{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImKdR3km31kQ",
    "outputId": "49db8fb7-d425-446e-c8e6-d3651e2cf656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.10.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy==1.10.1) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hydra-core in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hydra-core) (4.9.3)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hydra-core) (2.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hydra-core) (24.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "ERROR: Invalid requirement: '#'\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaex in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.17.0)\n",
      "Requirement already satisfied: vaex-astro<0.10,>=0.9.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.9.3)\n",
      "Requirement already satisfied: vaex-jupyter<0.9,>=0.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.8.2)\n",
      "Requirement already satisfied: vaex-ml<0.19,>=0.18.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.18.3)\n",
      "Requirement already satisfied: vaex-viz<0.6,>=0.5.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.5.4)\n",
      "Requirement already satisfied: vaex-core~=4.17.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (4.17.1)\n",
      "Requirement already satisfied: vaex-server~=0.9.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.9.0)\n",
      "Requirement already satisfied: vaex-hdf5<0.15,>=0.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex) (0.14.1)\n",
      "Requirement already satisfied: astropy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-astro<0.10,>=0.9.3->vaex) (6.0.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (3.13.4)\n",
      "Requirement already satisfied: aplus in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (0.11.0)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (1.16.0)\n",
      "Requirement already satisfied: progressbar2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (4.4.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (6.0.1)\n",
      "Requirement already satisfied: dask!=2022.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (2024.4.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.3.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=1.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (2.7.0)\n",
      "Requirement already satisfied: blake3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (0.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (2.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (13.7.1)\n",
      "Requirement already satisfied: tabulate>=0.8.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (0.9.0)\n",
      "Requirement already satisfied: frozendict!=2.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (2.4.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-core~=4.17.1->vaex) (16.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (24.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (7.1.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (0.12.1)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (8.1.7)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (1.4.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask!=2022.4.0->vaex-core~=4.17.1->vaex) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click>=8.1->dask!=2022.4.0->vaex-core~=4.17.1->vaex) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata>=4.13.0->dask!=2022.4.0->vaex-core~=4.17.1->vaex) (3.18.1)\n",
      "Requirement already satisfied: locket in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from partd>=1.2.0->dask!=2022.4.0->vaex-core~=4.17.1->vaex) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.8.0->vaex-core~=4.17.1->vaex) (4.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.8.0->vaex-core~=4.17.1->vaex) (2.18.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.8.0->vaex-core~=4.17.1->vaex) (0.6.0)\n",
      "Requirement already satisfied: h5py>=2.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-hdf5<0.15,>=0.13.0->vaex) (3.11.0)\n",
      "Requirement already satisfied: ipyvuetify<2,>=1.2.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (1.9.4)\n",
      "Requirement already satisfied: ipyleaflet in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (0.18.2)\n",
      "Requirement already satisfied: ipympl in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (0.9.4)\n",
      "Requirement already satisfied: ipyvolume>=0.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (0.6.3)\n",
      "Requirement already satisfied: xarray in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (2024.3.0)\n",
      "Requirement already satisfied: bqplot>=0.10.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-jupyter<0.9,>=0.8.2->vaex) (0.12.43)\n",
      "Requirement already satisfied: traitlets>=4.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (5.14.3)\n",
      "Requirement already satisfied: traittypes>=0.0.6 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.2.1)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (8.1.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (3.8.4)\n",
      "Requirement already satisfied: ipyvue>=1.7.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (1.11.0)\n",
      "Requirement already satisfied: ipywebrtc in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (0.6.0)\n",
      "Requirement already satisfied: pythreejs>=2.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (2.4.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (3.0.10)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (8.23.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (4.0.10)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (2.17.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.6.3)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.1.7)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (1.2.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (3.0.43)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->vaex-core~=4.17.1->vaex) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->vaex-core~=4.17.1->vaex) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->vaex-core~=4.17.1->vaex) (2024.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.2.13)\n",
      "Requirement already satisfied: ipydatawidgets>=1.1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pythreejs>=2.4.0->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (4.3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-ml<0.19,>=0.18.3->vaex) (3.1.3)\n",
      "Requirement already satisfied: numba in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-ml<0.19,>=0.18.3->vaex) (0.59.1)\n",
      "Requirement already satisfied: tornado>4.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-server~=0.9.0->vaex) (6.4)\n",
      "Requirement already satisfied: fastapi in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-server~=0.9.0->vaex) (0.110.2)\n",
      "Requirement already satisfied: cachetools in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-server~=0.9.0->vaex) (5.3.3)\n",
      "Requirement already satisfied: uvicorn[standard] in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaex-server~=0.9.0->vaex) (0.29.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (1.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (3.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->ipyvolume>=0.4->vaex-jupyter<0.9,>=0.8.2->vaex) (4.51.0)\n",
      "Requirement already satisfied: astropy-iers-data>=0.2024.2.26.0.28.55 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astropy->vaex-astro<0.10,>=0.9.3->vaex) (0.2024.4.15.2.45.49)\n",
      "Requirement already satisfied: pyerfa>=2.0.1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astropy->vaex-astro<0.10,>=0.9.3->vaex) (2.0.1.4)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi->vaex-server~=0.9.0->vaex) (0.37.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.38.0,>=0.37.2->fastapi->vaex-server~=0.9.0->vaex) (4.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi->vaex-server~=0.9.0->vaex) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi->vaex-server~=0.9.0->vaex) (1.3.1)\n",
      "Requirement already satisfied: branca>=0.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyleaflet->vaex-jupyter<0.9,>=0.8.2->vaex) (0.7.1)\n",
      "Requirement already satisfied: xyzservices>=2021.8.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipyleaflet->vaex-jupyter<0.9,>=0.8.2->vaex) (2024.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->vaex-ml<0.19,>=0.18.3->vaex) (2.1.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipympl->vaex-jupyter<0.9,>=0.8.2->vaex) (0.2.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba->vaex-ml<0.19,>=0.18.3->vaex) (0.42.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from progressbar2->vaex-core~=4.17.1->vaex) (3.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaex-core~=4.17.1->vaex) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaex-core~=4.17.1->vaex) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaex-core~=4.17.1->vaex) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->vaex-core~=4.17.1->vaex) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->vaex-core~=4.17.1->vaex) (0.1.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.0->bqplot>=0.10.1->vaex-jupyter<0.9,>=0.8.2->vaex) (0.2.2)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]->vaex-server~=0.9.0->vaex) (0.14.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]->vaex-server~=0.9.0->vaex) (1.0.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]->vaex-server~=0.9.0->vaex) (0.6.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]->vaex-server~=0.9.0->vaex) (12.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]->vaex-server~=0.9.0->vaex) (0.21.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install scipy==1.10.1\n",
    "!pip install hydra-core\n",
    "!pip install vaex\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OhRw8YOn31kR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import hydra\n",
    "import json\n",
    "import random\n",
    "from rich.pretty import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.feather\n",
    "import vaex\n",
    "import torch\n",
    "from typing import Callable, Iterable, Optional, Sequence\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "from rich.progress import track\n",
    "\n",
    "import gensim.models.word2vec as w2v\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import auc, average_precision_score, roc_curve\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "from transformers import RobertaModel, AutoConfig\n",
    "\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJy_FVar31kS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Librarires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87gtQ7v431kT"
   },
   "source": [
    "## Define Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4XIzj0dv31kT"
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 1\n",
    "max_batch_size = 1\n",
    "\n",
    "model_name = \"SciBERT\"    # \"RoBERTa\" or \"MedBERT\"  or \"SciBERT\"  or \"PubMedBERT\"\n",
    "\n",
    "train_split = 85595\n",
    "val_split = 12227\n",
    "test_split = 24456\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q1x5KIz31kT"
   },
   "source": [
    "## Definen Required Column Names and Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mNfHam0Z31kU"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "\n",
    "\n",
    "ID_COLUMN = \"_id\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"target\"\n",
    "SUBJECT_ID_COLUMN = \"subject_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Suk-fv31kU"
   },
   "source": [
    "## Define the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "c1uW1sAP31kU"
   },
   "outputs": [],
   "source": [
    "pretrained_model_directory = \"\"   # Define the SciBERT directory here\n",
    "preprocessed_data_file_directory = \"\"  #Define the preprocessed data file directory here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPzM4Br431kU"
   },
   "source": [
    "## Pretrained Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hJcl_2Lc31kU"
   },
   "outputs": [],
   "source": [
    "pretrained_model_path = Path(pretrained_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kq63bLw31kU"
   },
   "source": [
    "## Visualize the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "id": "KSXdBi8531kU",
    "outputId": "174e4ffa-055d-4df4-cba3-e83ffcad7c45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>_id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>text</th>\n",
       "      <th>icd10_proc</th>\n",
       "      <th>icd10_diag</th>\n",
       "      <th>target</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000084-DS-17</td>\n",
       "      <td>10000084</td>\n",
       "      <td>23052089</td>\n",
       "      <td>DS</td>\n",
       "      <td>17</td>\n",
       "      <td>2160-11-25 00:00:00</td>\n",
       "      <td>2160-11-25 15:09:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[E78.5, F02.80, G31.83, R29.6, R44.1, Z85.46]</td>\n",
       "      <td>[E78.5, F02.80, G31.83, R29.6, R44.1, Z85.46]</td>\n",
       "      <td>1305</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000117-DS-21</td>\n",
       "      <td>10000117</td>\n",
       "      <td>22927623</td>\n",
       "      <td>DS</td>\n",
       "      <td>21</td>\n",
       "      <td>2181-11-15 00:00:00</td>\n",
       "      <td>2181-11-15 15:04:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[F41.9, I34.1, K21.9, K31.819, K44.9, M81.0, R...</td>\n",
       "      <td>[F41.9, I34.1, K21.9, K31.819, K44.9, M81.0, R...</td>\n",
       "      <td>626</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000117-DS-22</td>\n",
       "      <td>10000117</td>\n",
       "      <td>27988844</td>\n",
       "      <td>DS</td>\n",
       "      <td>22</td>\n",
       "      <td>2183-09-21 00:00:00</td>\n",
       "      <td>2183-09-29 16:23:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[0QS734Z]</td>\n",
       "      <td>[E78.00, F41.9, G43.909, I34.1, K21.9, M81.0, ...</td>\n",
       "      <td>[0QS734Z, E78.00, F41.9, G43.909, I34.1, K21.9...</td>\n",
       "      <td>887</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000980-DS-24</td>\n",
       "      <td>10000980</td>\n",
       "      <td>25911675</td>\n",
       "      <td>DS</td>\n",
       "      <td>24</td>\n",
       "      <td>2191-05-24 00:00:00</td>\n",
       "      <td>2191-05-24 17:29:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[D50.0, E11.8, I12.9, I25.10, I25.2, I50.23, I...</td>\n",
       "      <td>[D50.0, E11.8, I12.9, I25.10, I25.2, I50.23, I...</td>\n",
       "      <td>1722</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000980-DS-25</td>\n",
       "      <td>10000980</td>\n",
       "      <td>29659838</td>\n",
       "      <td>DS</td>\n",
       "      <td>25</td>\n",
       "      <td>2191-07-19 00:00:00</td>\n",
       "      <td>2191-07-22 09:37:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[D63.1, E11.21, E78.5, I12.9, I25.10, I25.2, I...</td>\n",
       "      <td>[D63.1, E11.21, E78.5, I12.9, I25.10, I25.2, I...</td>\n",
       "      <td>1547</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122273</th>\n",
       "      <td>19999784-DS-7</td>\n",
       "      <td>19999784</td>\n",
       "      <td>26194817</td>\n",
       "      <td>DS</td>\n",
       "      <td>7</td>\n",
       "      <td>2119-07-02 00:00:00</td>\n",
       "      <td>2119-07-03 16:21:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[07DR3ZX, 0CJS8ZZ]</td>\n",
       "      <td>[C85.99, D47.2, D64.9, D72.819, E44.0, F17.210...</td>\n",
       "      <td>[07DR3ZX, 0CJS8ZZ, C85.99, D47.2, D64.9, D72.8...</td>\n",
       "      <td>2152</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122274</th>\n",
       "      <td>19999784-DS-8</td>\n",
       "      <td>19999784</td>\n",
       "      <td>24935234</td>\n",
       "      <td>DS</td>\n",
       "      <td>8</td>\n",
       "      <td>2119-07-12 00:00:00</td>\n",
       "      <td>2119-07-14 21:18:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[D47.2, F17.210, G95.89, J38.01, M54.16, R13.1...</td>\n",
       "      <td>[D47.2, F17.210, G95.89, J38.01, M54.16, R13.1...</td>\n",
       "      <td>1284</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122275</th>\n",
       "      <td>19999784-DS-9</td>\n",
       "      <td>19999784</td>\n",
       "      <td>23664472</td>\n",
       "      <td>DS</td>\n",
       "      <td>9</td>\n",
       "      <td>2119-08-09 00:00:00</td>\n",
       "      <td>2119-08-09 19:16:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[00UT0JZ, 02HV33Z, 0HQ6XZZ, 3E03305]</td>\n",
       "      <td>[C83.39, D47.2, E43., G96.0, G97.82, T81.32XA,...</td>\n",
       "      <td>[00UT0JZ, 02HV33Z, 0HQ6XZZ, 3E03305, C83.39, D...</td>\n",
       "      <td>1436</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122276</th>\n",
       "      <td>19999828-DS-6</td>\n",
       "      <td>19999828</td>\n",
       "      <td>29734428</td>\n",
       "      <td>DS</td>\n",
       "      <td>6</td>\n",
       "      <td>2147-08-04 00:00:00</td>\n",
       "      <td>2147-08-12 15:36:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[02HV33Z, 0HBHXZZ, 0HBJXZZ, 0HR7X74, 3E0436Z]</td>\n",
       "      <td>[B96.20, D68.51, E11.9, F12.90, F41.9, I10., I...</td>\n",
       "      <td>[02HV33Z, 0HBHXZZ, 0HBJXZZ, 0HR7X74, 3E0436Z, ...</td>\n",
       "      <td>1689</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122277</th>\n",
       "      <td>19999828-DS-7</td>\n",
       "      <td>19999828</td>\n",
       "      <td>25744818</td>\n",
       "      <td>DS</td>\n",
       "      <td>7</td>\n",
       "      <td>2149-01-18 00:00:00</td>\n",
       "      <td>2149-01-19 07:03:00</td>\n",
       "      <td>name unit no admission date discharge date dat...</td>\n",
       "      <td>[05HY33Z, 0J980ZZ, 0WPF0JZ]</td>\n",
       "      <td>[B95.4, D68.2, E11.10, E60., E87.6, F41.9, I10...</td>\n",
       "      <td>[05HY33Z, 0J980ZZ, 0WPF0JZ, B95.4, D68.2, E11....</td>\n",
       "      <td>1456</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122278 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               note_id  subject_id       _id note_type  note_seq  \\\n",
       "0       10000084-DS-17    10000084  23052089        DS        17   \n",
       "1       10000117-DS-21    10000117  22927623        DS        21   \n",
       "2       10000117-DS-22    10000117  27988844        DS        22   \n",
       "3       10000980-DS-24    10000980  25911675        DS        24   \n",
       "4       10000980-DS-25    10000980  29659838        DS        25   \n",
       "...                ...         ...       ...       ...       ...   \n",
       "122273   19999784-DS-7    19999784  26194817        DS         7   \n",
       "122274   19999784-DS-8    19999784  24935234        DS         8   \n",
       "122275   19999784-DS-9    19999784  23664472        DS         9   \n",
       "122276   19999828-DS-6    19999828  29734428        DS         6   \n",
       "122277   19999828-DS-7    19999828  25744818        DS         7   \n",
       "\n",
       "                  charttime            storetime  \\\n",
       "0       2160-11-25 00:00:00  2160-11-25 15:09:00   \n",
       "1       2181-11-15 00:00:00  2181-11-15 15:04:00   \n",
       "2       2183-09-21 00:00:00  2183-09-29 16:23:00   \n",
       "3       2191-05-24 00:00:00  2191-05-24 17:29:00   \n",
       "4       2191-07-19 00:00:00  2191-07-22 09:37:00   \n",
       "...                     ...                  ...   \n",
       "122273  2119-07-02 00:00:00  2119-07-03 16:21:00   \n",
       "122274  2119-07-12 00:00:00  2119-07-14 21:18:00   \n",
       "122275  2119-08-09 00:00:00  2119-08-09 19:16:00   \n",
       "122276  2147-08-04 00:00:00  2147-08-12 15:36:00   \n",
       "122277  2149-01-18 00:00:00  2149-01-19 07:03:00   \n",
       "\n",
       "                                                     text  \\\n",
       "0       name unit no admission date discharge date dat...   \n",
       "1       name unit no admission date discharge date dat...   \n",
       "2       name unit no admission date discharge date dat...   \n",
       "3       name unit no admission date discharge date dat...   \n",
       "4       name unit no admission date discharge date dat...   \n",
       "...                                                   ...   \n",
       "122273  name unit no admission date discharge date dat...   \n",
       "122274  name unit no admission date discharge date dat...   \n",
       "122275  name unit no admission date discharge date dat...   \n",
       "122276  name unit no admission date discharge date dat...   \n",
       "122277  name unit no admission date discharge date dat...   \n",
       "\n",
       "                                           icd10_proc  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                           [0QS734Z]   \n",
       "3                                                  []   \n",
       "4                                                  []   \n",
       "...                                               ...   \n",
       "122273                             [07DR3ZX, 0CJS8ZZ]   \n",
       "122274                                             []   \n",
       "122275           [00UT0JZ, 02HV33Z, 0HQ6XZZ, 3E03305]   \n",
       "122276  [02HV33Z, 0HBHXZZ, 0HBJXZZ, 0HR7X74, 3E0436Z]   \n",
       "122277                    [05HY33Z, 0J980ZZ, 0WPF0JZ]   \n",
       "\n",
       "                                               icd10_diag  \\\n",
       "0           [E78.5, F02.80, G31.83, R29.6, R44.1, Z85.46]   \n",
       "1       [F41.9, I34.1, K21.9, K31.819, K44.9, M81.0, R...   \n",
       "2       [E78.00, F41.9, G43.909, I34.1, K21.9, M81.0, ...   \n",
       "3       [D50.0, E11.8, I12.9, I25.10, I25.2, I50.23, I...   \n",
       "4       [D63.1, E11.21, E78.5, I12.9, I25.10, I25.2, I...   \n",
       "...                                                   ...   \n",
       "122273  [C85.99, D47.2, D64.9, D72.819, E44.0, F17.210...   \n",
       "122274  [D47.2, F17.210, G95.89, J38.01, M54.16, R13.1...   \n",
       "122275  [C83.39, D47.2, E43., G96.0, G97.82, T81.32XA,...   \n",
       "122276  [B96.20, D68.51, E11.9, F12.90, F41.9, I10., I...   \n",
       "122277  [B95.4, D68.2, E11.10, E60., E87.6, F41.9, I10...   \n",
       "\n",
       "                                                   target  num_words  \\\n",
       "0           [E78.5, F02.80, G31.83, R29.6, R44.1, Z85.46]       1305   \n",
       "1       [F41.9, I34.1, K21.9, K31.819, K44.9, M81.0, R...        626   \n",
       "2       [0QS734Z, E78.00, F41.9, G43.909, I34.1, K21.9...        887   \n",
       "3       [D50.0, E11.8, I12.9, I25.10, I25.2, I50.23, I...       1722   \n",
       "4       [D63.1, E11.21, E78.5, I12.9, I25.10, I25.2, I...       1547   \n",
       "...                                                   ...        ...   \n",
       "122273  [07DR3ZX, 0CJS8ZZ, C85.99, D47.2, D64.9, D72.8...       2152   \n",
       "122274  [D47.2, F17.210, G95.89, J38.01, M54.16, R13.1...       1284   \n",
       "122275  [00UT0JZ, 02HV33Z, 0HQ6XZZ, 3E03305, C83.39, D...       1436   \n",
       "122276  [02HV33Z, 0HBHXZZ, 0HBJXZZ, 0HR7X74, 3E0436Z, ...       1689   \n",
       "122277  [05HY33Z, 0J980ZZ, 0WPF0JZ, B95.4, D68.2, E11....       1456   \n",
       "\n",
       "        num_targets  \n",
       "0                 6  \n",
       "1                 9  \n",
       "2                13  \n",
       "3                22  \n",
       "4                16  \n",
       "...             ...  \n",
       "122273           12  \n",
       "122274            8  \n",
       "122275           14  \n",
       "122276           27  \n",
       "122277           21  \n",
       "\n",
       "[122278 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather(preprocessed_data_file_directory)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niYmERYP31kV"
   },
   "source": [
    "## Split the Data into Training, Validation and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8_cylIuX31kV"
   },
   "outputs": [],
   "source": [
    "def assign_splits(dataframe, train_split, val_split, test_split):\n",
    "    unique_ids = dataframe['_id'].unique()\n",
    "    total_ids = len(unique_ids)\n",
    "    total_split = train_split + val_split + test_split\n",
    "\n",
    "    if total_split > total_ids:\n",
    "        raise ValueError(\"Total splits exceed the total number of unique IDs.\")\n",
    "\n",
    "    train_ids = unique_ids[:train_split]\n",
    "    val_ids = unique_ids[train_split:train_split+val_split]\n",
    "    test_ids = unique_ids[train_split+val_split:total_split]\n",
    "\n",
    "    split_df = pd.DataFrame({'_id': np.concatenate([train_ids, val_ids, test_ids]),\n",
    "                             'split': ['train'] * train_split + ['val'] * val_split + ['test'] * test_split})\n",
    "\n",
    "    return split_df\n",
    "\n",
    "split_df = assign_splits(df, train_split=train_split, val_split=val_split, test_split=test_split)\n",
    "split_df.to_feather(\"split_data.feather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psjxhcRO31kV"
   },
   "source": [
    "### Visualize the Splitted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wodf3UoJ31kV",
    "outputId": "9cc38cbc-052c-476d-8e6a-747a80fb8308",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 85595\n",
      "Val count: 12227\n",
      "Val count: 24456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              _id  split\n",
       "0       25352398  train\n",
       "1       20407963  train\n",
       "2       29710796  train\n",
       "3       29712230  train\n",
       "4       25936632  train\n",
       "...          ...    ...\n",
       "122273  29634509   test\n",
       "122274  25083567   test\n",
       "122275  28167262   test\n",
       "122276  23814306   test\n",
       "122277  21456551   test\n",
       "\n",
       "[122278 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather(\"split_data.feather\")\n",
    "# Counting occurrences of each label\n",
    "label_counts = df['split'].value_counts()\n",
    "\n",
    "# Printing the counts\n",
    "print(\"Train count:\", label_counts.get('train', 0))\n",
    "print(\"Val count:\", label_counts.get('val', 0))\n",
    "print(\"Val count:\", label_counts.get('test', 0))\n",
    "\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMr-1WQY31kW"
   },
   "source": [
    "## Define the Preprocessed Data Filename and Split Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SKZsDpoO31kW"
   },
   "outputs": [],
   "source": [
    "data_filename = preprocessed_data_file_directory\n",
    "split_filename = \"split_data.feather\"\n",
    "code_column_names = [\"icd10_diag\", \"icd10_proc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whRmuMon31kW"
   },
   "source": [
    "# **Get the Data From Data PipeLine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkRkLH3Y31kW"
   },
   "source": [
    "### Define the Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u4_xOJEB31kW"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lookups:\n",
    "    data_info: dict[str, Any]\n",
    "    code2description: Optional[dict[str, str]] = None\n",
    "    code_system2code_indices: Optional[dict[str, torch.Tensor]] = None\n",
    "    split2code_indices: Optional[dict[str, torch.Tensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    \"\"\"Dataclass containing the dataset and the code occurrences of each code system.\"\"\"\n",
    "\n",
    "    df: pa.Table\n",
    "    code_system2code_counts: dict[str, dict[str, int]]\n",
    "\n",
    "    @property\n",
    "    def train(self) -> list[pa.RecordBatch]:\n",
    "        \"\"\"Get the training data.\n",
    "\n",
    "        Returns:\n",
    "            list[pa.RecordBatch]: List of record batches.\n",
    "        \"\"\"\n",
    "        batches = (\n",
    "            self.df.filter(pc.field(\"split\") == \"train\")\n",
    "            .sort_by(\"num_words\")\n",
    "            .to_batches(max_chunksize=1)\n",
    "        )\n",
    "        return self.from_batches_to_list(batches)\n",
    "\n",
    "    @property\n",
    "    def val(self) -> list[pa.RecordBatch]:\n",
    "        \"\"\"Get the validation data.\n",
    "\n",
    "        Returns:\n",
    "            list[pa.RecordBatch]: List of record batches.\n",
    "        \"\"\"\n",
    "        batches = (\n",
    "            self.df.filter(pc.field(\"split\") == \"val\")\n",
    "            .sort_by(\"num_words\")\n",
    "            .to_batches(max_chunksize=1)\n",
    "        )\n",
    "        return self.from_batches_to_list(batches)\n",
    "\n",
    "    @property\n",
    "    def test(self) -> list[pa.RecordBatch]:\n",
    "        \"\"\"Get the test data.\n",
    "\n",
    "        Returns:\n",
    "            list[pa.RecordBatch]: List of record batches.\n",
    "        \"\"\"\n",
    "        batches = (\n",
    "            self.df.filter(pc.field(\"split\") == \"test\")\n",
    "            .sort_by(\"num_words\")\n",
    "            .to_batches(max_chunksize=1)\n",
    "        )\n",
    "        return self.from_batches_to_list(batches)\n",
    "\n",
    "    def from_batches_to_list(\n",
    "        self,\n",
    "        batches: list[pa.RecordBatch],\n",
    "    ) -> list[tuple[torch.Tensor, np.array, str, int, torch.Tensor]]:\n",
    "        \"\"\"Convert a list of record batches to a list of tuples\n",
    "\n",
    "        Args:\n",
    "            batches (list[pa.RecordBatch]): List of record batches.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[torch.Tensor, np.array, str, int, torch.Tensor]]: List of tuples.\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        import pdb\n",
    "\n",
    "        for batch in track(batches, description=\"Creating examples\"):\n",
    "            token_ids = torch.from_numpy(\n",
    "                batch.column(\"token_ids\")\n",
    "                .flatten()\n",
    "                .to_numpy(zero_copy_only=False, writable=True)\n",
    "            )\n",
    "            targets = (\n",
    "                batch.column(TARGET_COLUMN)\n",
    "                .flatten()\n",
    "                .to_numpy(zero_copy_only=False, writable=True)\n",
    "            )\n",
    "            id = batch.column(ID_COLUMN)[0].as_py()\n",
    "            num_tokens = len(token_ids)\n",
    "            attention_mask = torch.ones_like(token_ids)\n",
    "            examples.append((token_ids, targets, id, num_tokens, attention_mask))\n",
    "        return examples\n",
    "\n",
    "    @property\n",
    "    def get_documents(self) -> list[str]:\n",
    "        \"\"\"Get all the documents in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of documents.\n",
    "        \"\"\"\n",
    "        return self.df.column(TEXT_COLUMN).to_pylist()\n",
    "\n",
    "    @property\n",
    "    def all_target_counts(self) -> dict[str, int]:\n",
    "        \"\"\"Get the number of occurrences of each code in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, int]: Dictionary with the number of occurrences of each code.\n",
    "        \"\"\"\n",
    "        return reduce(lambda x, y: {**x, **y}, self.code_system2code_counts.values())\n",
    "\n",
    "    @property\n",
    "    def get_train_documents(self) -> list[str]:\n",
    "        \"\"\"Get the training documents.\"\"\"\n",
    "        return (\n",
    "            self.df.filter(pc.field(\"split\") == \"train\").column(TEXT_COLUMN).to_pylist()\n",
    "        )\n",
    "\n",
    "    def split_targets(self, name: str) -> set[str]:\n",
    "        \"\"\"Get the targets of a split.\"\"\"\n",
    "        return set(\n",
    "            self.df.filter(pc.field(\"split\") == name)\n",
    "            .column(TARGET_COLUMN)\n",
    "            .combine_chunks()\n",
    "            .flatten()\n",
    "            .unique()\n",
    "            .to_pylist()\n",
    "        )\n",
    "\n",
    "    def split_size(self, name: str) -> int:\n",
    "        \"\"\"Get the size of a split.\"\"\"\n",
    "        return len(self.df.filter(pc.field(\"split\") == name))\n",
    "\n",
    "    def num_split_targets(self, name: str) -> int:\n",
    "        \"\"\"Get the number of targets of a split.\"\"\"\n",
    "        return len(self.split_targets(name))\n",
    "\n",
    "    @property\n",
    "    def all_targets(self) -> set[str]:\n",
    "        \"\"\"Get all the targets in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            set[str]: Set of all targets.\n",
    "        \"\"\"\n",
    "        all_codes = set()\n",
    "        for codesystem in self.code_system2code_counts.values():\n",
    "            all_codes |= set(codesystem.keys())\n",
    "        return all_codes\n",
    "\n",
    "    @property\n",
    "    def info(self) -> dict[str, int]:\n",
    "        \"\"\"Get information about the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, int]: Dictionary with information about the dataset.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"num_classes\": len(self.all_targets),\n",
    "            \"num_examples\": len(self.df),\n",
    "            \"num_train_tokens\": sum(\n",
    "                self.df.filter(pc.field(\"split\") == \"train\")\n",
    "                .column(\"num_words\")\n",
    "                .to_pylist()\n",
    "            ),\n",
    "            \"average_tokens_per_example\": sum(self.df.column(\"num_words\").to_pylist())\n",
    "            / len(self.df),\n",
    "            \"num_train_examples\": self.split_size(\"train\"),\n",
    "            \"num_val_examples\": self.split_size(\"val\"),\n",
    "            \"num_test_examples\": self.split_size(\"test\"),\n",
    "            \"num_train_classes\": self.num_split_targets(\"train\"),\n",
    "            \"num_val_classes\": self.num_split_targets(\"val\"),\n",
    "            \"num_test_classes\": self.num_split_targets(\"test\"),\n",
    "            \"average_classes_per_example\": sum(\n",
    "                [\n",
    "                    sum(codesystem.values())\n",
    "                    for codesystem in self.code_system2code_counts.values()\n",
    "                ]\n",
    "            )\n",
    "            / len(self.df),\n",
    "        }\n",
    "\n",
    "    def truncate_text(self, max_length: int) -> None:\n",
    "        \"\"\"Truncate text to a maximum length.\n",
    "\n",
    "        Args:\n",
    "            max_length (int): Maximum length of text.\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            return\n",
    "\n",
    "        text = self.df.column(TEXT_COLUMN)\n",
    "        text_split = pc.utf8_split_whitespace(text)  # pylint: disable=no-member\n",
    "        text_split_df = text_split.to_pandas().apply(lambda x: x[:max_length])\n",
    "        text_split_truncate = pa.array(text_split_df.values)\n",
    "        text_truncate = pc.binary_join(  # pylint: disable=no-member\n",
    "            text_split_truncate, \" \"\n",
    "        )\n",
    "\n",
    "        # Change column in table\n",
    "        new_table_no_text = self.df.drop([TEXT_COLUMN])\n",
    "        new_table = new_table_no_text.append_column(\n",
    "            pa.field(TEXT_COLUMN, pa.string()), text_truncate\n",
    "        )\n",
    "        del self.df\n",
    "        self.df = new_table\n",
    "\n",
    "    def transform_text(self, batch_transform: Callable[[list[str]], str]) -> None:\n",
    "        \"\"\"Transform the text using a batch transform function.\n",
    "\n",
    "        Args:\n",
    "            batch_transform (Callable[[list[str]], str]): Batch transform function.\n",
    "        \"\"\"\n",
    "        token_ids_list = []\n",
    "        for batch in track(\n",
    "            self.df.to_batches(max_chunksize=10000), description=\"Transforming text...\"\n",
    "        ):\n",
    "            texts = batch.column(TEXT_COLUMN).to_pylist()\n",
    "            token_ids_list += batch_transform(texts)\n",
    "\n",
    "        # Convert to list of pyarrays\n",
    "        token_ids = pa.array(token_ids_list, type=pa.list_(pa.int64()))\n",
    "        del token_ids_list\n",
    "        # Append to table\n",
    "        new_table_no_text = self.df.drop([TEXT_COLUMN])\n",
    "        del self.df\n",
    "        new_table = new_table_no_text.append_column(\n",
    "            pa.field(\"token_ids\", pa.list_(pa.int64())), [token_ids]\n",
    "        )\n",
    "        self.df = new_table\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    \"\"\"Batch class. Used to store a batch of data.\"\"\"\n",
    "\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    ids: torch.Tensor\n",
    "    code_descriptions: Optional[torch.Tensor] = None\n",
    "    num_tokens: Optional[torch.Tensor] = None\n",
    "    attention_mask: Optional[torch.Tensor] = None\n",
    "\n",
    "    def to(self, device: Any) -> \"Batch\":\n",
    "        \"\"\"Move the batch to a device.\n",
    "\n",
    "        Args:\n",
    "            device (Any): Device to move the batch to.\n",
    "\n",
    "        Returns:\n",
    "            self: Moved batch.\n",
    "        \"\"\"\n",
    "        self.data = self.data.to(device, non_blocking=True)\n",
    "        self.targets = self.targets.to(device, non_blocking=True)\n",
    "        if self.attention_mask is not None:\n",
    "            self.attention_mask = self.attention_mask.to(device, non_blocking=True)\n",
    "        return self\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.data = self.data.pin_memory()\n",
    "        self.targets = self.targets.pin_memory()\n",
    "        if self.attention_mask is not None:\n",
    "            self.attention_mask = self.attention_mask.pin_memory()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6367UwN-31kX"
   },
   "source": [
    "## Data Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ucpkz9Kt31kX"
   },
   "outputs": [],
   "source": [
    "def get_code_system2code_counts(\n",
    "    df: vaex.dataframe.DataFrame, code_systems: list[str]\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        df (vaex.dataframe.DataFrame): The dataset in vaex dataframe format\n",
    "        code_systems (list[str]): list of code systems to get counts for\n",
    "    Returns:\n",
    "        dict[str, dict[str, int]]: A dictionary with code systems as keys and a dictionary of code counts as values\n",
    "    \"\"\"\n",
    "    code_system2code_counts = defaultdict(dict)\n",
    "    for col in code_systems:\n",
    "        codes = df[col].values.flatten().value_counts().to_pylist()\n",
    "        code_system2code_counts[col] = {\n",
    "            code[\"values\"]: code[\"counts\"] for code in codes\n",
    "        }\n",
    "    return code_system2code_counts\n",
    "\n",
    "\n",
    "def data_pipeline(data_file_name,\n",
    "                 code_column_name,\n",
    "                 split_file_name):\n",
    "\n",
    "    with vaex.cache.memory_infinite():  # type: ignore\n",
    "        df = vaex.from_arrow_table(\n",
    "            pyarrow.feather.read_table(\n",
    "                data_filename,\n",
    "                columns=[\n",
    "                    ID_COLUMN,\n",
    "                    TEXT_COLUMN,\n",
    "                    TARGET_COLUMN,\n",
    "                    \"num_words\",\n",
    "                    \"num_targets\",\n",
    "                ]\n",
    "                + code_column_names,\n",
    "            )\n",
    "        )\n",
    "        splits = vaex.from_arrow_table(\n",
    "            pyarrow.feather.read_table(\n",
    "                split_filename,\n",
    "            )\n",
    "        )\n",
    "        df = df.join(splits, on=ID_COLUMN, how=\"inner\")\n",
    "        code_system2code_counts = get_code_system2code_counts(\n",
    "            df, code_column_names\n",
    "        )\n",
    "        schema = pa.schema(\n",
    "            [\n",
    "                pa.field(ID_COLUMN, pa.int64()),\n",
    "                pa.field(TEXT_COLUMN, pa.large_utf8()),\n",
    "                pa.field(TARGET_COLUMN, pa.list_(pa.large_string())),\n",
    "                pa.field(\"split\", pa.large_string()),\n",
    "                pa.field(\"num_words\", pa.int64()),\n",
    "                pa.field(\"num_targets\", pa.int64()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return Data(\n",
    "            df[\n",
    "                [\n",
    "                    ID_COLUMN,\n",
    "                    TEXT_COLUMN,\n",
    "                    TARGET_COLUMN,\n",
    "                    \"split\",\n",
    "                    \"num_words\",\n",
    "                    \"num_targets\",\n",
    "                ]\n",
    "            ]\n",
    "            .to_arrow_table()\n",
    "            .cast(schema),\n",
    "            code_system2code_counts,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFIZI-5Y31kY"
   },
   "source": [
    "## Select and Print the Device(GPU or CPU) for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "K6yDu1NQ31kY",
    "outputId": "3219f61d-5212-4a57-b217-3fca4f575ecd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Device: cuda'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Device: cuda'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxwGGchb31kY"
   },
   "source": [
    "## Get Data from Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wCNnCp5931kY"
   },
   "outputs": [],
   "source": [
    "data = data_pipeline(data_file_name = data_filename,\n",
    "                 code_column_name = code_column_names,\n",
    "                 split_file_name = split_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPtDZXxb31kY"
   },
   "source": [
    "# **Text Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joomn5Gx31kY"
   },
   "source": [
    "## Text Encoder Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "X1zNB9wh31kY"
   },
   "outputs": [],
   "source": [
    "class BaseTextEncoder:\n",
    "    \"\"\"The base class for text encoders.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def save(self, path: Path) -> None:\n",
    "        \"\"\"Save the text encoder.\n",
    "\n",
    "        Args:\n",
    "            path (Path): The path to save the text encoder to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> \"BaseTextEncoder\":\n",
    "        \"\"\"Load the text encoder.\n",
    "\n",
    "        Args:\n",
    "            path (Path): The path to load the text encoder from.\n",
    "\n",
    "        Returns:\n",
    "            TextEncoder: The text encoder.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, texts: Iterable[str]) -> None:\n",
    "        \"\"\"Fit the text encoder.\n",
    "\n",
    "        Args:\n",
    "            texts (Iterable[str]): The texts to fit the text encoder to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def token2index(self) -> dict:\n",
    "        \"\"\"The token to index mapping.\n",
    "\n",
    "        Returns:\n",
    "            dict: The token to index mapping.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def index2token(self) -> dict:\n",
    "        \"\"\"The index to token mapping.\n",
    "\n",
    "        Returns:\n",
    "            dict: The index to token mapping.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> np.ndarray:\n",
    "        \"\"\"The weights of the text encoder.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The weights of the text encoder.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "def word_tokenizer(string):\n",
    "    \"\"\"\n",
    "    Splits a string by whitespace characters.\n",
    "    Args:\n",
    "        string (string): The string to be split by whitespace characters.\n",
    "    Returns:\n",
    "        list: The words of the string.\n",
    "    \"\"\"\n",
    "    return string.split()\n",
    "\n",
    "class Word2Vec(BaseTextEncoder):\n",
    "    \"\"\"The word2vec text encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, config, model: Optional[w2v.Word2Vec] = None) -> None:\n",
    "        super().__init__(config)\n",
    "        self.model = model\n",
    "        self.tokeniser = word_tokenizer\n",
    "\n",
    "    def save(self, path: Path) -> None:\n",
    "        \"\"\"Save the word2vec model.\n",
    "\n",
    "        Args:\n",
    "            path (Path): The path to save the word2vec model to.\n",
    "        \"\"\"\n",
    "        self.model.save(str(path))\n",
    "        OmegaConf.save(config=self.config, f=path.with_suffix(\".yaml\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> \"Word2Vec\":\n",
    "        \"\"\"Load the word2vec model.\n",
    "\n",
    "        Args:\n",
    "            path (Path): The path to load the word2vec model from.\n",
    "\n",
    "        Returns:\n",
    "            Word2Vec: The word2vec model.\n",
    "        \"\"\"\n",
    "        model = w2v.Word2Vec.load(str(path))\n",
    "        config = OmegaConf.load(path.with_suffix(\".yaml\"))\n",
    "        return cls(config, model=model)\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> np.ndarray:\n",
    "        return self.model.wv.vectors\n",
    "\n",
    "    @property\n",
    "    def token2index(self) -> dict:\n",
    "        \"\"\"The token to index mapping.\n",
    "\n",
    "        Returns:\n",
    "            dict: The token to index mapping.\n",
    "        \"\"\"\n",
    "        return self.model.wv.key_to_index\n",
    "\n",
    "    @property\n",
    "    def index2token(self) -> dict:\n",
    "        \"\"\"The index to token mapping.\n",
    "\n",
    "        Returns:\n",
    "            dict: The index to token mapping.\n",
    "        \"\"\"\n",
    "        return self.model.wv.index_to_key\n",
    "\n",
    "    @property\n",
    "    def embedding_size(self) -> int:\n",
    "        \"\"\"The embedding size.\n",
    "\n",
    "        Returns:\n",
    "            int: The embedding size.\n",
    "        \"\"\"\n",
    "        return self.model.vector_size\n",
    "\n",
    "    def fit(self, texts: Iterable[str], model_configs) -> None:\n",
    "        \"\"\"Fit the word2vec model.\n",
    "\n",
    "        Args:\n",
    "            texts (Iterable[str]): The texts to fit the word2vec model to.\n",
    "        \"\"\"\n",
    "        sentences = [self.tokeniser(sentence) for sentence in texts]\n",
    "        self.model = w2v.Word2Vec(sentences, **model_configs)\n",
    "        self.remove_rare_words(texts, 3)\n",
    "        vec = np.random.randn(self.embedding_size)\n",
    "        self.model.wv.add_vector(UNKNOWN_TOKEN, vec)\n",
    "        self.normalize_weights()\n",
    "        self.model.wv.add_vector(PAD_TOKEN, np.zeros(self.embedding_size))\n",
    "\n",
    "    def normalize_weights(self) -> None:\n",
    "        \"\"\"Normalize the word2vec model.\"\"\"\n",
    "        self.model.init_sims(replace=True)\n",
    "\n",
    "    def remove_rare_words(self, texts: Iterable[str], min_document_count: int) -> None:\n",
    "        \"\"\"Remove the rare words from the word2vec model.\n",
    "\n",
    "        Args:\n",
    "            texts (Iterable[str]): The data to train on. Each element is a sentence.\n",
    "            min_document_count (int): The minimum number of documents a word occurs in.\n",
    "        \"\"\"\n",
    "\n",
    "        word_counts = dict()\n",
    "        for sentence in texts:\n",
    "            words_in_sentence = set()\n",
    "            for word in sentence.split():\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 0\n",
    "                if word not in words_in_sentence:\n",
    "                    word_counts[word] += 1\n",
    "                words_in_sentence.add(word)\n",
    "\n",
    "        words_to_remove = [\n",
    "            word\n",
    "            for word, count in word_counts.items()\n",
    "            if (count < min_document_count) and (word in self.model.wv.key_to_index)\n",
    "        ]\n",
    "        ids_to_remove = [self.model.wv.key_to_index[word] for word in words_to_remove]\n",
    "\n",
    "        for word in words_to_remove:\n",
    "            del self.model.wv.key_to_index[word]\n",
    "\n",
    "        self.model.wv.vectors = np.delete(self.model.wv.vectors, ids_to_remove, axis=0)\n",
    "\n",
    "        for i in sorted(ids_to_remove, reverse=True):\n",
    "            del self.model.wv.index_to_key[i]\n",
    "\n",
    "        for i, key in enumerate(self.model.wv.index_to_key):\n",
    "            self.model.wv.key_to_index[key] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "76E5QpRf31kZ"
   },
   "outputs": [],
   "source": [
    "text_encoder_configs = {\n",
    "    \"min_document_count\": 3\n",
    "}\n",
    "model_configs = {\n",
    "        \"vector_size\": 100,\n",
    "        \"min_count\": 0,\n",
    "        \"workers\": 4,\n",
    "        \"epochs\": 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "T3oDzOYf31kZ"
   },
   "outputs": [],
   "source": [
    "def get_text_encoder( texts):\n",
    "    path = Path(\"word2vec_full.model\")\n",
    "    text_encoder = Word2Vec(config=text_encoder_configs)\n",
    "    text_encoder.fit(texts, model_configs)\n",
    "    text_encoder.save(path)\n",
    "    return text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81ParT2Q31kZ"
   },
   "source": [
    "## Get the Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaMuneTm31kZ",
    "outputId": "60722cb3-63f8-43d9-8e24-8ac36d20d3cb"
   },
   "outputs": [],
   "source": [
    "text_encoder = get_text_encoder(texts=data.get_train_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBYCQa9c31kZ"
   },
   "source": [
    "# **Get Label and Text Transforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYgVpw7m31kZ"
   },
   "source": [
    "### Transform Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8XUUj9b31kZ"
   },
   "outputs": [],
   "source": [
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.unknown_token = UNKNOWN_TOKEN\n",
    "        self.pad_token = PAD_TOKEN\n",
    "        self.pad_index = 0\n",
    "        self.unknown_index = None\n",
    "\n",
    "    def forward(self, x: Any):\n",
    "        return self.transform(x)\n",
    "\n",
    "    def transform(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def seq2batch(self, sequence) -> torch.Tensor:\n",
    "        \"\"\"A batching function for the Transform classes. Used in the collate function of the dataset.\n",
    "\n",
    "        Args:\n",
    "            sequence (Sequence[torch.Tensor]): A Sequence of vectors of equal lengths.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of vectors.\n",
    "        \"\"\"\n",
    "        return torch.stack(sequence)\n",
    "\n",
    "    def batch_transform(self, texts: list[str]) -> list[list[int]]:\n",
    "        \"\"\"Transform a batch of texts into a batch of indices.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): A batch of texts.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A batch of indices.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OneHotEncoder(Transform):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"One hot encoder for targets\"\"\"\n",
    "        super().__init__()\n",
    "        self.target2index = {}\n",
    "        self.index2target = {}\n",
    "        self.file_name = \"target2index.json\"\n",
    "\n",
    "    def fit(self, targets: set[str]) -> None:\n",
    "        \"\"\"Fit the encoder to all the targets in the dataset. That also includes the validation and test set.\n",
    "\n",
    "        Args:\n",
    "            targets (set[str]): List of targets\n",
    "        \"\"\"\n",
    "        for index, target in enumerate(targets):\n",
    "            self.target2index[target] = index\n",
    "            self.index2target[index] = target\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Number of classes supported by the encoder\n",
    "\n",
    "        Returns:\n",
    "            int: Number of classes\n",
    "        \"\"\"\n",
    "        return len(self.target2index)\n",
    "\n",
    "    def get_classes(self) -> list[str]:\n",
    "        \"\"\"Get the list of classes supported by the encoder. The classes are sorted by their index.\"\"\"\n",
    "        return [self.index2target[index] for index in range(self.num_classes)]\n",
    "\n",
    "    def transform(self, targets: Iterable[str]) -> torch.Tensor:\n",
    "        \"\"\"Transform a set of targets into a one-hot encoded tensor\n",
    "\n",
    "        Args:\n",
    "            targets (set[str]): Set of targets\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: One-hot encoded tensor\n",
    "        \"\"\"\n",
    "        output_tensor = torch.zeros(self.num_classes)\n",
    "        for label in targets:\n",
    "            if label in self.target2index:\n",
    "                output_tensor[self.target2index[label]] = 1\n",
    "        return output_tensor\n",
    "\n",
    "    def inverse_transform(self, output_tensor: torch.Tensor) -> set[str]:\n",
    "        \"\"\"Transform a one-hot encoded tensor into a set of targets\n",
    "\n",
    "        Args:\n",
    "            output_tensor (torch.Tensor): One-hot encoded tensor\n",
    "\n",
    "        Returns:\n",
    "            set[str]: Set of targets\n",
    "        \"\"\"\n",
    "\n",
    "        indices = torch.nonzero(output_tensor).squeeze(0).numpy()\n",
    "        return set([self.index2target[int(index)] for index in indices])\n",
    "\n",
    "    def get_indices(self, targets: Iterable[str]) -> torch.Tensor:\n",
    "        \"\"\"Get the indices of the targets\n",
    "\n",
    "        Args:\n",
    "            targets (Iterable[str]): Set of targets\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Indices of the targets\n",
    "        \"\"\"\n",
    "        indices = torch.zeros(len(targets), dtype=torch.long)\n",
    "        for index, label in enumerate(targets):\n",
    "            if label in self.target2index:\n",
    "                indices[index] = self.target2index[label]\n",
    "        return indices\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save target2index as a json file\n",
    "\n",
    "        Args:\n",
    "            path (str): path to save the json file\n",
    "        \"\"\"\n",
    "        path = Path(path) / self.file_name\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.target2index, f)\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Load target2index from a json file\n",
    "\n",
    "        Args:\n",
    "            path (str): path of the directory containing the json file\n",
    "        \"\"\"\n",
    "        path = Path(path) / self.file_name\n",
    "        with open(path, \"r\") as f:\n",
    "            self.target2index = json.load(f)\n",
    "            self.index2target = {v: k for k, v in self.target2index.items()}\n",
    "\n",
    "\n",
    "class HuggingFaceTokenizer(Transform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        add_special_token: bool = True,\n",
    "        padding: bool = False,\n",
    "        truncation: bool = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"A class to encode text into indices and back.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the huggingface tokenizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.add_special_token = add_special_token\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.truncation = truncation\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, **kwargs)\n",
    "\n",
    "    def transform(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Transform a text into a list of indices.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of indices.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            add_special_tokens=self.add_special_token,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def batch_transform(self, texts: list[str]) -> list[list[int]]:\n",
    "        \"\"\"Transform a batch of texts into a batch of indices.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): A batch of texts.\n",
    "\n",
    "        Returns:\n",
    "            list[list[int]]: A batch of indices.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            add_special_tokens=self.add_special_token,\n",
    "            max_length=self.max_length,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "    def inverse_transform(self, indices: list[str]) -> list[str]:\n",
    "        \"\"\"Transform a list of indices into a list of tokens.\n",
    "\n",
    "        Args:\n",
    "            indices (torch.Tensor): A tensor of indices.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.tokenizer.decode(indices)\n",
    "\n",
    "    def seq2batch(\n",
    "        self, sequence: Sequence[torch.Tensor], chunk_size: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Batch a sequence of vectors of different lengths. Use the pad_index to pad the vectors.\n",
    "\n",
    "        Args:\n",
    "            sequences (Sequence[torch.Tensor]): A sequence of sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A batched tensor.\n",
    "        \"\"\"\n",
    "        if chunk_size == 0:\n",
    "            return torch.nn.utils.rnn.pad_sequence(\n",
    "                sequence, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        sequence = list(sequence)\n",
    "        batch_size = len(sequence)\n",
    "        max_length = max([len(x) for x in sequence])\n",
    "        if max_length % chunk_size != 0:\n",
    "            max_length = max_length + (chunk_size - max_length % chunk_size)\n",
    "\n",
    "        # pad first sequence to the desired length\n",
    "        sequence[0] = torch.nn.functional.pad(\n",
    "            sequence[0],\n",
    "            (0, max_length - len(sequence[0])),\n",
    "            value=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        return (\n",
    "            torch.nn.utils.rnn.pad_sequence(\n",
    "                sequence, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            .contiguous()\n",
    "            .view((batch_size, -1, chunk_size))\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"The size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    def fit(self, texts: list[str]) -> None:\n",
    "        pass\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIGryVYS31ka"
   },
   "source": [
    "## Transform Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-DioCYP31ka"
   },
   "outputs": [],
   "source": [
    "def get_transform_targets(\n",
    "    targets: Optional[set[str]] = data.all_targets,\n",
    "    text_encoder: Optional[BaseTextEncoder] = None):\n",
    "\n",
    "    transform_class = OneHotEncoder()\n",
    "\n",
    "    if text_encoder:\n",
    "        transform_class.set_tokenmap(\n",
    "            token2index=text_encoder.token2index, index2token=text_encoder.index2token\n",
    "        )\n",
    "\n",
    "\n",
    "    elif targets:\n",
    "        transform_class.fit(targets)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provide set of labels, a text encoder or texts of tokens to perform fit transformation\"\n",
    "        )\n",
    "\n",
    "    return transform_class\n",
    "\n",
    "def get_transform_texts(\n",
    "    targets: Optional[set[str]] = None,\n",
    "    texts: Optional[list[str]] = None,\n",
    "    text_encoder: Optional[BaseTextEncoder] = None,\n",
    "    load_transform_path: Optional[str] = None,):\n",
    "\n",
    "    transform_class = HuggingFaceTokenizer(model_path = pretrained_model_path ,\n",
    "                                          add_special_token = True,\n",
    "                                          padding = False,\n",
    "                                          max_length = None,\n",
    "                                           use_fast = True,\n",
    "                                           do_lower_case = True,\n",
    "                                           truncation = False\n",
    "                                          )\n",
    "    if load_transform_path:\n",
    "        transform_class.load(load_transform_path)\n",
    "        print(\"loaded transform\")\n",
    "\n",
    "    elif text_encoder:\n",
    "        transform_class.set_tokenmap(\n",
    "            token2index=text_encoder.token2index, index2token=text_encoder.index2token\n",
    "        )\n",
    "\n",
    "    elif texts:\n",
    "        transform_class.fit(texts)\n",
    "\n",
    "    elif targets:\n",
    "        transform_class.fit(targets)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provide set of labels, a text encoder or texts of tokens to perform fit transformation\"\n",
    "        )\n",
    "\n",
    "    return transform_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqeXrkng31ka"
   },
   "source": [
    "## Get Label and Text Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtpyS2eJ31ka",
    "outputId": "af3c111d-8185-4408-c575-c835bb47b442"
   },
   "outputs": [],
   "source": [
    "label_transform = get_transform_targets()\n",
    "text_transform = get_transform_texts(\n",
    "        texts=data.get_train_documents,\n",
    "        text_encoder=text_encoder,\n",
    "        load_transform_path=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfo7SKC_31ka"
   },
   "source": [
    "## Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33,
     "referenced_widgets": [
      "c80a736fbda340c587a54019e5ef61e5",
      "77b0fc3528174d0b8f75502530a4f9d3"
     ]
    },
    "id": "P1-gWJZE31kb",
    "outputId": "6ee1e264-c134-43a5-c394-d77cba865ab3"
   },
   "outputs": [],
   "source": [
    "data.truncate_text(4000)\n",
    "data.transform_text(text_transform.batch_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tebrl8T031kb"
   },
   "source": [
    "## Lookup Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJmWydMU31kb"
   },
   "outputs": [],
   "source": [
    "def get_data_info(data: Data, vocab_size: int, pad_index: int) -> dict:\n",
    "    data_info = data.info\n",
    "    data_info[\"vocab_size\"] = vocab_size\n",
    "    data_info[\"pad_index\"] = pad_index\n",
    "    return data_info\n",
    "\n",
    "def get_code_system2code_indices(\n",
    "    data: Data, label_transform: Transform\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    code_system2code_indices = {}\n",
    "    for codesystem, codes in data.code_system2code_counts.items():\n",
    "        code_system2code_indices[codesystem] = label_transform.get_indices(\n",
    "            set(codes.keys())\n",
    "        )\n",
    "    return code_system2code_indices\n",
    "\n",
    "\n",
    "def get_split2code_indices(\n",
    "    data: Data, label_transform: Transform\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    split2code_indices = {}\n",
    "    split2code_indices[\"train\"] = label_transform.get_indices(\n",
    "        data.split_targets(\"train\")\n",
    "    )\n",
    "    split2code_indices[\"train_val\"] = label_transform.get_indices(\n",
    "        data.split_targets(\"train\")\n",
    "    )\n",
    "    split2code_indices[\"val\"] = label_transform.get_indices(data.split_targets(\"val\"))\n",
    "    split2code_indices[\"test\"] = label_transform.get_indices(data.split_targets(\"test\"))\n",
    "    return split2code_indices\n",
    "\n",
    "\n",
    "def load_lookups(\n",
    "    data: Data,\n",
    "    label_transform: Transform,\n",
    "    text_transform: Transform,\n",
    ") -> Lookups:\n",
    "    \"\"\"Load the lookups.\n",
    "\n",
    "    Args:\n",
    "        config (OmegaConf): The config.\n",
    "\n",
    "    Returns:\n",
    "        Lookups: The lookups.\n",
    "    \"\"\"\n",
    "    data_info = get_data_info(\n",
    "        data, text_transform.vocab_size, label_transform.pad_index\n",
    "    )\n",
    "    code_system2code_indices = get_code_system2code_indices(data, label_transform)\n",
    "    split2code_indices = get_split2code_indices(data, label_transform)\n",
    "\n",
    "    return Lookups(\n",
    "        data_info=data_info,\n",
    "        code_system2code_indices=code_system2code_indices,\n",
    "        split2code_indices=split2code_indices,\n",
    "    )\n",
    "\n",
    "def get_lookups(\n",
    "    data: Data,\n",
    "    label_transform,\n",
    "    text_transform,\n",
    ") -> Lookups:\n",
    "    return load_lookups(\n",
    "        data=data,\n",
    "        label_transform=label_transform,\n",
    "        text_transform=text_transform,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9sASo4E31kb"
   },
   "source": [
    "## Get Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYekCkik31kb"
   },
   "outputs": [],
   "source": [
    "lookups = get_lookups(\n",
    "        data=data,\n",
    "        label_transform=label_transform,\n",
    "        text_transform=text_transform,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH_l--y31kb"
   },
   "source": [
    "## **Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ek7UMa3d31kb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "class LabelAttention(nn.Module):\n",
    "    def __init__(self, input_size: int, projection_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.first_linear = nn.Linear(input_size, projection_size, bias=False)\n",
    "        self.second_linear = nn.Linear(projection_size, num_classes, bias=False)\n",
    "        self.third_linear = nn.Linear(input_size, num_classes)\n",
    "        self._init_weights(mean=0.0, std=0.03)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"LAAT attention mechanism\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch_size, seq_len, input_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        weights = torch.tanh(self.first_linear(x))\n",
    "        att_weights = self.second_linear(weights)\n",
    "        att_weights = self.layer_norm(att_weights)\n",
    "        att_weights = torch.nn.functional.softmax(att_weights, dim=1).transpose(1, 2)\n",
    "        weighted_output = att_weights @ x\n",
    "        return (\n",
    "            self.third_linear.weight.mul(weighted_output)\n",
    "            .sum(dim=2)\n",
    "            .add(self.third_linear.bias)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self, mean: float = 0.0, std: float = 0.03) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the weights\n",
    "\n",
    "        Args:\n",
    "            mean (float, optional): Mean of the normal distribution. Defaults to 0.0.\n",
    "            std (float, optional): Standard deviation of the normal distribution. Defaults to 0.03.\n",
    "        \"\"\"\n",
    "\n",
    "        torch.nn.init.normal_(self.first_linear.weight, mean, std)\n",
    "        torch.nn.init.normal_(self.second_linear.weight, mean, std)\n",
    "        torch.nn.init.normal_(self.third_linear.weight, mean, std)\n",
    "\n",
    "\n",
    "class PLMICD(nn.Module):\n",
    "    def __init__(self, num_classes: int, model_path: str, **kwargs):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_path, num_labels=num_classes, finetuning_task=None\n",
    "        )\n",
    "        self.model = BertModel.from_pretrained(model_path, config=self.config)\n",
    "        self.attention = LabelAttention(\n",
    "            input_size=self.config.hidden_size,\n",
    "            projection_size=self.config.hidden_size,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        self.loss = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    def get_loss(self, logits, targets):\n",
    "        return self.loss(logits, targets)\n",
    "\n",
    "    def training_step(self, batch) -> dict[str, torch.Tensor]:\n",
    "        data, targets, attention_mask = batch.data, batch.targets, batch.attention_mask\n",
    "        logits = self(data, attention_mask)\n",
    "        loss = self.get_loss(logits, targets)\n",
    "        logits = torch.sigmoid(logits)\n",
    "        return {\"logits\": logits, \"loss\": loss, \"targets\": targets}\n",
    "\n",
    "    def validation_step(self, batch) -> dict[str, torch.Tensor]:\n",
    "        data, targets, attention_mask = batch.data, batch.targets, batch.attention_mask\n",
    "        logits = self(data, attention_mask)\n",
    "        loss = self.get_loss(logits, targets)\n",
    "        logits = torch.sigmoid(logits)\n",
    "        return {\"logits\": logits, \"loss\": loss, \"targets\": targets}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        input_ids (torch.LongTensor of shape (batch_size, num_chunks, chunk_size))\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, num_labels)`, `optional`):\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_chunks, chunk_size = input_ids.size()\n",
    "        outputs = self.model(\n",
    "            input_ids.view(-1, chunk_size),\n",
    "            attention_mask=attention_mask.view(-1, chunk_size)\n",
    "            if attention_mask is not None\n",
    "            else None,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        hidden_output = outputs[0].view(batch_size, num_chunks * chunk_size, -1)\n",
    "        logits = self.attention(hidden_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSR8DNdR31kc"
   },
   "outputs": [],
   "source": [
    "def get_model(data_info: dict, text_encoder: Optional[Any] = None):\n",
    "    model_class = PLMICD(model_path = pretrained_model_path , text_encoder = text_encoder, **data_info)\n",
    "    return model_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19m9EDSe31kc"
   },
   "source": [
    "## Get the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e89aHXsQ31kc"
   },
   "outputs": [],
   "source": [
    "model = get_model(data_info=lookups.data_info, text_encoder=text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "e0ZHZ3fB31kc",
    "outputId": "555fbcc8-9758-4dad-a3bb-4a57f9b76826"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "# print data info\n",
    "pprint(lookups.data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZogG5QC31kc"
   },
   "source": [
    "## Metric Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iriG1ZUy31kc"
   },
   "outputs": [],
   "source": [
    "def detach(x):\n",
    "    \"\"\"Detach a tensor from the computational graph\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach()\n",
    "    return x\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    base_tags = set()\n",
    "    _str_value_fmt = \"<.3\"\n",
    "    higher_is_better = True\n",
    "    batch_update = True\n",
    "    filter_codes = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        tags: set,\n",
    "        number_of_classes: int,\n",
    "        threshold: Optional[float] = None,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.tags = self.base_tags if tags is None else (tags | self.base_tags)\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.device = \"cpu\"\n",
    "        self.threshold = threshold\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        \"\"\"Update the metric from a batch\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def set_target_boolean_indices(self, target_boolean_indices: list[bool]):\n",
    "        self.target_boolean_indices = target_boolean_indices\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Compute the metric value\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the metric\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def to(self, device: str):\n",
    "        self.device = device\n",
    "        if self.threshold is not None:\n",
    "            self.threshold = torch.tensor(self.threshold).clone().to(device)\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def set_number_of_classes(self, number_of_classes: int):\n",
    "        self.number_of_classes = number_of_classes\n",
    "\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics: list[Metric],\n",
    "        code_indices: Optional[torch.Tensor] = None,\n",
    "        code_system_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.metrics = metrics\n",
    "        self.code_system_name = code_system_name\n",
    "        if code_indices is not None:\n",
    "            # Get overlapping indices\n",
    "            self.code_indices = code_indices.clone()\n",
    "            self.set_number_of_classes(len(code_indices))\n",
    "        else:\n",
    "            self.code_indices = None\n",
    "        self.reset()\n",
    "\n",
    "    def set_number_of_classes(self, number_of_classes_split: int):\n",
    "        \"\"\"Sets the number of classes for metrics with the filter_codes attribute to the number of classes in the split.\n",
    "        Args:\n",
    "            number_of_classes_split (int): Number of classes in the split\n",
    "        \"\"\"\n",
    "        for metric in self.metrics:\n",
    "            if metric.filter_codes:\n",
    "                metric.set_number_of_classes(number_of_classes_split)\n",
    "\n",
    "    def to(self, device: str):\n",
    "        self.metrics = [metric.to(device) for metric in self.metrics]\n",
    "        if self.code_indices is not None:\n",
    "            self.code_indices = self.code_indices.to(device)\n",
    "        return self\n",
    "\n",
    "    def filter_batch(self, batch: dict) -> dict:\n",
    "        if self.code_indices is None:\n",
    "            return batch\n",
    "\n",
    "        filter_batch = {}\n",
    "        targets, logits = batch[\"targets\"], batch[\"logits\"]\n",
    "\n",
    "        filtered_targets = torch.index_select(targets, -1, self.code_indices)\n",
    "        filtered_logits = torch.index_select(logits, -1, self.code_indices)\n",
    "        # Elements in the batch with targets\n",
    "        idx_targets = torch.sum(filtered_targets, dim=-1) > 0\n",
    "        # Remove all elements wihtout targets\n",
    "        filter_batch[\"targets\"] = filtered_targets[idx_targets]\n",
    "        filter_batch[\"logits\"] = filtered_logits[idx_targets]\n",
    "        return filter_batch\n",
    "\n",
    "    def filter_tensor(\n",
    "        self, tensor: torch.Tensor, code_indices: torch.Tensor\n",
    "    ) -> list[torch.Tensor]:\n",
    "        if code_indices is None:\n",
    "            return tensor\n",
    "        return torch.index_select(tensor, -1, code_indices)\n",
    "\n",
    "    def is_best(\n",
    "        self,\n",
    "        prev_best: Optional[torch.Tensor],\n",
    "        current: torch.Tensor,\n",
    "        higher_is_better: bool,\n",
    "    ) -> bool:\n",
    "        if higher_is_better:\n",
    "            return prev_best is None or current > prev_best\n",
    "        else:\n",
    "            return prev_best is None or current < prev_best\n",
    "\n",
    "    def update_best_metrics(self, metric_dict: dict[str, torch.Tensor]):\n",
    "        for metric in self.metrics:\n",
    "            if metric.name not in metric_dict:\n",
    "                continue\n",
    "\n",
    "            if self.is_best(\n",
    "                self.best_metrics[metric.name],\n",
    "                metric_dict[metric.name],\n",
    "                metric.higher_is_better,\n",
    "            ):\n",
    "                self.best_metrics[metric.name] = metric_dict[metric.name]\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        for metric in self.metrics:\n",
    "            if metric.batch_update and not metric.filter_codes:\n",
    "                metric.update(batch)\n",
    "\n",
    "        filtered_batch = self.filter_batch(batch)\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            if metric.batch_update and metric.filter_codes:\n",
    "                metric.update(filtered_batch)\n",
    "\n",
    "    def compute(\n",
    "        self,\n",
    "        logits: Optional[torch.Tensor] = None,\n",
    "        targets: Optional[torch.Tensor] = None,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        metric_dict = {\n",
    "            metric.name: metric.compute()\n",
    "            for metric in self.metrics\n",
    "            if metric.batch_update\n",
    "        }\n",
    "        if logits is not None and targets is not None:\n",
    "            # Compute the metrics for the whole dataset\n",
    "            if self.code_indices is not None:\n",
    "                logits_filtered = self.filter_tensor(logits, self.code_indices.cpu())\n",
    "                targets_filtered = self.filter_tensor(targets, self.code_indices.cpu())\n",
    "\n",
    "            for metric in self.metrics:\n",
    "                if metric.batch_update:\n",
    "                    continue\n",
    "                if metric.filter_codes and self.code_indices is not None:\n",
    "                    metric_dict[metric.name] = metric.compute(\n",
    "                        logits=logits_filtered, targets=targets_filtered\n",
    "                    )\n",
    "                else:\n",
    "                    metric_dict[metric.name] = metric.compute(\n",
    "                        logits=logits, targets=targets\n",
    "                    )\n",
    "\n",
    "            metric_dict.update(\n",
    "                {\n",
    "                    metric.name: metric.compute(logits=logits, targets=targets)\n",
    "                    for metric in self.metrics\n",
    "                    if not metric.batch_update\n",
    "                }\n",
    "            )\n",
    "        self.update_best_metrics(metric_dict)\n",
    "        return metric_dict\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_metrics()\n",
    "        self.best_metrics = {metric.name: None for metric in self.metrics}\n",
    "\n",
    "    def get_best_metric(self, metric_name: str) -> dict[str, torch.Tensor]:\n",
    "        return self.best_metrics[metric_name]\n",
    "\n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def set_threshold(self, threshold: float):\n",
    "        for metric in self.metrics:\n",
    "            if hasattr(metric, \"threshold\"):\n",
    "                metric.threshold = threshold\n",
    "\n",
    "\n",
    "\"\"\" ------------Classification Metrics-------------\"\"\"\n",
    "\n",
    "\n",
    "class ExactMatchRatio(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = 0.5,\n",
    "        name: str = \"exact_match_ratio\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: int = 0,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        predictions = (logits > self.threshold).long()\n",
    "        self._num_exact_matches += torch.all(\n",
    "            torch.eq(predictions, targets), dim=-1\n",
    "        ).sum()\n",
    "        self._num_examples += targets.size(0)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._num_exact_matches / self._num_examples\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_exact_matches = torch.tensor(0).to(self.device)\n",
    "        self._num_examples = 0\n",
    "\n",
    "\n",
    "class Recall(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_classes: int,\n",
    "        threshold: float = 0.5,\n",
    "        average: str = \"micro\",\n",
    "        name: str = \"recall\",\n",
    "        tags: set[str] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if average:\n",
    "            name = f\"{name}_{average}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        self._average = average\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        predictions = (logits > self.threshold).long()\n",
    "        self._tp += torch.sum(predictions * targets, dim=0)\n",
    "        self._fn += torch.sum((1 - predictions) * targets, dim=0)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        if self._average == \"micro\":\n",
    "            return (self._tp.sum() / (self._tp.sum() + self._fn.sum() + 1e-10)).cpu()\n",
    "        if self._average == \"macro\":\n",
    "            return torch.mean(self._tp / (self._tp + self._fn + 1e-10)).cpu()\n",
    "        if self._average is None or self._average == \"none\":\n",
    "            return (self._tp / (self._tp + self._fn + 1e-10)).cpu()\n",
    "        raise ValueError(f\"Invalid average: {self._average}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._tp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "        self._fn = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "\n",
    "\n",
    "class Precision(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_classes: int,\n",
    "        threshold: float = 0.5,\n",
    "        average: str = \"micro\",\n",
    "        name: str = \"precision\",\n",
    "        tags: set[str] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if average:\n",
    "            name = f\"{name}_{average}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        self._average = average\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        predictions = (logits > self.threshold).long()\n",
    "        self._tp += torch.sum(predictions * targets, dim=0)\n",
    "        self._fp += torch.sum((predictions) * (1 - targets), dim=0)\n",
    "\n",
    "    def compute(self):\n",
    "        if self._average == \"micro\":\n",
    "            return (self._tp.sum() / (self._tp.sum() + self._fp.sum() + 1e-10)).cpu()\n",
    "        if self._average == \"macro\":\n",
    "            return torch.mean(self._tp / (self._tp + self._fp + 1e-10)).cpu()\n",
    "        if self._average is None or self._average == \"none\":\n",
    "            return (self._tp / (self._tp + self._fp + 1e-10)).cpu()\n",
    "        raise ValueError(f\"Invalid average: {self._average}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._tp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "        self._fp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "\n",
    "\n",
    "class FPR(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "    higher_is_better = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_classes: int,\n",
    "        threshold: float = 0.5,\n",
    "        average: str = \"micro\",\n",
    "        name: str = \"fpr\",\n",
    "        tags: set[str] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if average:\n",
    "            name = f\"{name}_{average}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        self._average = average\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        predictions = (logits > self.threshold).long()\n",
    "        self._fp += torch.sum(predictions * (1 - targets), dim=0)\n",
    "        self._tn += torch.sum((1 - predictions) * (1 - targets), dim=0)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        if self._average == \"micro\":\n",
    "            return (self._fp.sum() / (self._fp.sum() + self._tn.sum() + 1e-10)).cpu()\n",
    "        if self._average == \"macro\":\n",
    "            return torch.mean(self._fp / (self._fp + self._tn + 1e-10)).cpu()\n",
    "        if self._average is None or self._average == \"none\":\n",
    "            return (self._fp / (self._fp + self._tn + 1e-10)).cpu()\n",
    "        raise ValueError(f\"Invalid average: {self._average}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._fp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "        self._tn = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "\n",
    "\n",
    "class AUC(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "    batch_update = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        average: str = \"micro\",\n",
    "        name: str = \"auc\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        \"\"\"Area under the ROC curve. All classes that have no positive examples are ignored as implemented by Mullenbach et al. Please note that all the logits and targets are stored in the GPU memory if they have not already been moved to the CPU.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): logits from a machine learning model. [batch_size, num_classes]\n",
    "            name (str, optional): name of the metric. Defaults to \"auc\".\n",
    "            tags (set[str], optional): metrics tages. Defaults to None.\n",
    "            log_to_console (bool, optional): whether to print this metric. Defaults to True.\n",
    "            number_of_classes (int, optional): number of classes. Defaults to None.\n",
    "            filter_codes (bool, optional): whether to filter out codes that have no positive examples. Defaults to True.\n",
    "        \"\"\"\n",
    "        if average:\n",
    "            name = f\"{name}_{average}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "        self._average = average\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def compute(self, logits: torch.Tensor, targets: torch.Tensor) -> np.float32:\n",
    "        logits = detach(logits).numpy()\n",
    "        targets = detach(targets).numpy()\n",
    "        if self._average == \"micro\":\n",
    "            fprs, tprs, _ = self.roc_curve(\n",
    "                logits=logits, targets=targets, average=self._average\n",
    "            )\n",
    "            value = auc(fprs, tprs)\n",
    "        if self._average == \"macro\":\n",
    "            fprs, tprs, _ = self.roc_curve(\n",
    "                logits=logits, targets=targets, average=\"none\"\n",
    "            )\n",
    "            value = np.mean([auc(fpr, tpr) for fpr, tpr in zip(fprs, tprs)])\n",
    "        return value\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        raise NotImplementedError(\"AUC is not batch updateable.\")\n",
    "\n",
    "    def roc_curve(\n",
    "        self,\n",
    "        logits: list[torch.Tensor],\n",
    "        targets: list[torch.Tensor],\n",
    "        average: str = \"micro\",\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        thresholds = torch.linspace(0, 1, 1000)\n",
    "\n",
    "        if average == \"micro\":\n",
    "            return roc_curve(targets.ravel(), logits.ravel())\n",
    "        if average == \"none\":\n",
    "            fprs, tprs, thresholds = [], [], []\n",
    "            for i in range(targets.shape[1]):\n",
    "                if targets[:, i].sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                fpr, tpr, threshold = roc_curve(targets[:, i], logits[:, i])\n",
    "\n",
    "                if np.any(np.isnan(fpr)) or np.any(np.isnan(tpr)):\n",
    "                    continue\n",
    "\n",
    "                fprs.append(fpr)\n",
    "                tprs.append(tpr)\n",
    "                thresholds.append(threshold)\n",
    "\n",
    "            return fprs, tprs, thresholds\n",
    "        raise ValueError(f\"Invalid average: {average}\")\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class F1Score(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_classes: int,\n",
    "        threshold: float = 0.5,\n",
    "        average: str = \"micro\",\n",
    "        name: str = \"f1\",\n",
    "        tags: set[str] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if average:\n",
    "            name = f\"{name}_{average}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        self._average = average\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        predictions = (logits > self.threshold).long()\n",
    "        self._tp += torch.sum((predictions) * (targets), dim=0)\n",
    "        self._fp += torch.sum(predictions * (1 - targets), dim=0)\n",
    "        self._fn += torch.sum((1 - predictions) * targets, dim=0)\n",
    "\n",
    "    def compute(self):\n",
    "        if self._average == \"micro\":\n",
    "            return (\n",
    "                self._tp.sum()\n",
    "                / (self._tp.sum() + 0.5 * (self._fp.sum() + self._fn.sum()) + 1e-10)\n",
    "            ).cpu()\n",
    "        if self._average == \"macro\":\n",
    "            return torch.mean(\n",
    "                self._tp / (self._tp + 0.5 * (self._fp + self._fn) + 1e-10)\n",
    "            ).cpu()\n",
    "        if self._average is None or self._average == \"none\":\n",
    "            return (self._tp / (self._tp + 0.5 * (self._fp + self._fn) + 1e-10)).cpu()\n",
    "        raise ValueError(f\"Invalid average: {self._average}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._tp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "        self._fp = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "        self._fn = torch.zeros((self.number_of_classes)).to(self.device)\n",
    "\n",
    "\n",
    "\"\"\" ------------Information Retrieval Metrics-------------\"\"\"\n",
    "\n",
    "\n",
    "class PrecisionAtRecall(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"precision@recall\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        num_targets = targets.sum(dim=1, dtype=torch.int64)\n",
    "        _, indices = torch.sort(logits, dim=1, descending=True)\n",
    "        sorted_targets = targets.gather(1, indices)\n",
    "        sorted_targets_cum = torch.cumsum(sorted_targets, dim=1)\n",
    "        self._precision_sum += torch.sum(\n",
    "            sorted_targets_cum.gather(1, num_targets.unsqueeze(1) - 1).squeeze()\n",
    "            / num_targets\n",
    "        )\n",
    "        self._num_examples += logits.size(0)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._precision_sum.cpu() / self._num_examples\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_examples = 0\n",
    "        self._precision_sum = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "\n",
    "class Precision_K(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int = 10,\n",
    "        name: str = \"precision\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        name = f\"{name}@{k}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "        self._k = k\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        top_k = torch.topk(logits, dim=1, k=self._k)\n",
    "\n",
    "        targets_k = targets.gather(1, top_k.indices)\n",
    "        logits_k = torch.ones(targets_k.shape, device=targets_k.device)\n",
    "\n",
    "        tp = torch.sum(logits_k * targets_k, dim=1)\n",
    "        fp = torch.sum((logits_k) * (1 - targets_k), dim=1)\n",
    "        self._num_examples += logits.size(0)\n",
    "        self._precision_sum += torch.sum(tp / (tp + fp + 1e-10))\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._precision_sum.cpu() / self._num_examples\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_examples = 0\n",
    "        self._precision_sum = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "\n",
    "class MeanAveragePrecision(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"map\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        _, indices = torch.sort(logits, dim=1, descending=True)\n",
    "        sorted_targets = targets.gather(1, indices)\n",
    "        sorted_targets_cum = torch.cumsum(sorted_targets, dim=1)\n",
    "        batch_size = logits.size(0)\n",
    "        denom = torch.arange(1, targets.shape[1] + 1, device=targets.device).repeat(\n",
    "            batch_size, 1\n",
    "        )\n",
    "        prec_at_k = sorted_targets_cum / denom\n",
    "        average_precision_batch = torch.sum(\n",
    "            prec_at_k * sorted_targets, dim=1\n",
    "        ) / torch.sum(sorted_targets, dim=1)\n",
    "        self._average_precision_sum += torch.sum(average_precision_batch)\n",
    "        self._num_examples += batch_size\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._average_precision_sum.cpu() / self._num_examples\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_examples = 0\n",
    "        self._average_precision_sum = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "\n",
    "class Recall_K(Metric):\n",
    "    _str_value_fmt = \"6.4\"  # 6.4321\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int = 10,\n",
    "        name: str = \"recall\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = True,\n",
    "    ):\n",
    "        name = f\"{name}@{k}\"\n",
    "        if not filter_codes:\n",
    "            name = f\"{name}_mullenbach\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "        self._k = k\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        logits, targets = detach(batch[\"logits\"]), detach(batch[\"targets\"])\n",
    "        top_k = torch.topk(logits, dim=1, k=self._k)\n",
    "\n",
    "        targets_k = targets.gather(1, top_k.indices)\n",
    "        logits_k = torch.ones(targets_k.shape, device=targets_k.device)\n",
    "\n",
    "        tp = torch.sum(logits_k * targets_k, dim=1)\n",
    "        total_number_of_relevant_targets = torch.sum(targets, dim=1)\n",
    "\n",
    "        self._num_examples += logits.size(0)\n",
    "        self._recall_sum += torch.sum(tp / (total_number_of_relevant_targets + 1e-10))\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._recall_sum.cpu() / self._num_examples\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_examples = 0\n",
    "        self._recall_sum = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "\n",
    "\"\"\" ------------Running Mean Metrics-------------\"\"\"\n",
    "\n",
    "\n",
    "class RunningMeanMetric(Metric):\n",
    "    _str_value_fmt = \"<.3\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        tags: set[str],\n",
    "        number_of_classes: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Create a running mean metric.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the metric\n",
    "            tags (Set[str]): Tags to use for grouping with other metrics.\n",
    "            number_of_classes (Optional[int], optional): Number of classes. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name, tags=tags, number_of_classes=number_of_classes)\n",
    "\n",
    "    def update(self, batch: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_value(\n",
    "        self,\n",
    "        values: torch.Tensor,\n",
    "        reduce_by: Optional[torch.Tensor] = None,\n",
    "        weight_by: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            values (torch.Tensor): Values of the metric\n",
    "            reduce_by (Optional[torch.Tensor], optional): A single or per example divisor of the values. Defaults to batch size.\n",
    "            weight_by (Optional[torch.Tensor], optional): A single or per example weights for the running mean. Defaults to `reduce_by`.\n",
    "        \"\"\"\n",
    "        values = detach(values)\n",
    "        reduce_by = detach(reduce_by)\n",
    "\n",
    "        numel = values.numel() if isinstance(values, torch.Tensor) else 1\n",
    "        value = values.sum().tolist() if isinstance(values, torch.Tensor) else values\n",
    "\n",
    "        reduce_by = (\n",
    "            reduce_by.sum().tolist()\n",
    "            if isinstance(reduce_by, torch.Tensor)\n",
    "            else (reduce_by or numel)\n",
    "        )\n",
    "\n",
    "        weight_by = (\n",
    "            weight_by.sum().tolist()\n",
    "            if isinstance(weight_by, torch.Tensor)\n",
    "            else (weight_by or reduce_by)\n",
    "        )\n",
    "\n",
    "        values = value / reduce_by\n",
    "\n",
    "        d = self.weight_by + weight_by\n",
    "        w1 = self.weight_by / d\n",
    "        w2 = weight_by / d\n",
    "\n",
    "        self._values = (\n",
    "            self._values * w1 + values * w2\n",
    "        )  # Reduce between batches (over entire epoch)\n",
    "\n",
    "        self.weight_by = d\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self._values\n",
    "\n",
    "    def reset(self):\n",
    "        self._values = torch.tensor(0.0).to(self.device)\n",
    "        self.weight_by = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "\n",
    "class LossMetric(RunningMeanMetric):\n",
    "    base_tags = {\"losses\"}\n",
    "    higher_is_better = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"loss\",\n",
    "        tags: set[str] = None,\n",
    "        number_of_classes: Optional[int] = None,\n",
    "        filter_codes: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            tags=tags,\n",
    "            number_of_classes=number_of_classes,\n",
    "        )\n",
    "        self.filter_codes = filter_codes\n",
    "\n",
    "    def update(self, batch: dict[str, torch.Tensor]):\n",
    "        loss = detach(batch[\"loss\"]).cpu()\n",
    "        self.update_value(loss, reduce_by=loss.numel(), weight_by=loss.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRvyZuyi31kd"
   },
   "outputs": [],
   "source": [
    "def get_metric_collection(\n",
    "    config: OmegaConf,\n",
    "    number_of_classes: int,\n",
    "    code_system_code_indices: Optional[torch.Tensor] = None,\n",
    "    split_code_indices: Optional[torch.Tensor] = None,\n",
    "    code_system_name: Optional[str] = None,\n",
    ") :\n",
    "    metric_list = []\n",
    "    for metric in config:\n",
    "        metric_class = metric['name']\n",
    "        metric_list.append(\n",
    "            globals()[metric_class](number_of_classes=number_of_classes, **metric['configs'])\n",
    "        )\n",
    "\n",
    "    if code_system_code_indices is not None and split_code_indices is not None:\n",
    "        # Get overlapping indices\n",
    "        code_indices = torch.tensor(\n",
    "            np.intersect1d(code_system_code_indices, split_code_indices)\n",
    "        )\n",
    "    elif code_system_code_indices is not None:\n",
    "        code_indices = code_system_code_indices.clone()\n",
    "    elif split_code_indices is not None:\n",
    "        code_indices = split_code_indices.clone()\n",
    "    else:\n",
    "        code_indices = None\n",
    "\n",
    "    return MetricCollection(\n",
    "        metrics=metric_list,\n",
    "        code_indices=code_indices,\n",
    "        code_system_name=code_system_name,\n",
    "    )\n",
    "def get_metric_collections(\n",
    "    config: OmegaConf,\n",
    "    number_of_classes: int,\n",
    "    split_names: list[str] = [\"train\", \"train_val\", \"val\", \"test\"],\n",
    "    splits_with_multiple_code_systems: set[str] = {\"train_val\", \"val\", \"test\"},\n",
    "    code_system2code_indices: dict[str, torch.Tensor] = None,\n",
    "    split2code_indices: dict[str, torch.Tensor] = None,\n",
    ") :\n",
    "    metric_collections = defaultdict(dict)\n",
    "    for split_name in split_names:\n",
    "        if split2code_indices is not None:\n",
    "            split_code_indices = split2code_indices.get(split_name)\n",
    "        else:\n",
    "            split_code_indices = None\n",
    "\n",
    "        metric_collections[split_name][\"all\"] = get_metric_collection(\n",
    "            config=config,\n",
    "            number_of_classes=number_of_classes,\n",
    "            split_code_indices=split_code_indices,\n",
    "        )\n",
    "\n",
    "        if split_name not in splits_with_multiple_code_systems:\n",
    "            continue\n",
    "\n",
    "        if code_system2code_indices is None:\n",
    "            continue\n",
    "\n",
    "        for (\n",
    "            code_system_name,\n",
    "            code_system_code_indices,\n",
    "        ) in code_system2code_indices.items():\n",
    "            metric_collections[split_name][code_system_name] = get_metric_collection(\n",
    "                config=config,\n",
    "                number_of_classes=number_of_classes,\n",
    "                code_system_code_indices=code_system_code_indices,\n",
    "                split_code_indices=split_code_indices,\n",
    "                code_system_name=code_system_name,\n",
    "            )\n",
    "\n",
    "    return metric_collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBc-D3AZ31kd"
   },
   "source": [
    "## Evaluation Metrics Mentioned in Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTpX0tAY31ke"
   },
   "outputs": [],
   "source": [
    "metrics_config = [\n",
    "    {\"name\": \"F1Score\", \"configs\": {\"average\": \"micro\", \"threshold\": 0.5}},\n",
    "    {\"name\": \"Precision\", \"configs\": {\"average\": \"micro\", \"threshold\": 0.5}},\n",
    "    {\"name\": \"AUC\", \"configs\": {\"average\": \"micro\"}},\n",
    "    {\"name\": \"Precision_K\", \"configs\": {\"k\": 1}},\n",
    "    {\"name\": \"Precision_K\", \"configs\": {\"k\": 5}},\n",
    "    {\"name\": \"Precision_K\", \"configs\": {\"k\": 8}},\n",
    "    {\"name\": \"Precision_K\", \"configs\": {\"k\": 15}},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWnJeOK231ke"
   },
   "outputs": [],
   "source": [
    "metric_collections = get_metric_collections(\n",
    "        config=metrics_config,\n",
    "        number_of_classes=lookups.data_info[\"num_classes\"],\n",
    "        code_system2code_indices=lookups.code_system2code_indices,\n",
    "        split2code_indices=lookups.split2code_indices,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFUi15ZH31ke"
   },
   "source": [
    "## Dataset Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj-0SwL931ke"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class HuggingfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: list[pa.RecordBatch],\n",
    "        text_transform: Callable,\n",
    "        label_transform: Callable,\n",
    "        lookups: Lookups,\n",
    "        chunk_size: int = 128,\n",
    "        split_name: str = \"train\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.split_name = split_name\n",
    "        self.text_transform = text_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.chunk_size = chunk_size\n",
    "        self.lookups = lookups\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, int, int, torch.Tensor]:\n",
    "        token_ids, targets, id, num_tokens, attention_mask = self.data[idx]\n",
    "        targets = self.label_transform(targets)\n",
    "        return token_ids, targets, id, num_tokens, attention_mask\n",
    "\n",
    "    def collate_fn(self, batch: tuple[list, list, list, list]) -> Batch:\n",
    "        token_ids, targets, ids, num_tokens, attention_mask = zip(*batch)\n",
    "        data = self.text_transform.seq2batch(token_ids, chunk_size=self.chunk_size)\n",
    "        attention_mask = self.text_transform.seq2batch(\n",
    "            attention_mask, chunk_size=self.chunk_size\n",
    "        )\n",
    "        targets = self.label_transform.seq2batch(targets)\n",
    "        ids = torch.tensor(ids)\n",
    "        num_tokens = torch.tensor(num_tokens)\n",
    "        return Batch(\n",
    "            data=data,\n",
    "            targets=targets,\n",
    "            ids=ids,\n",
    "            num_tokens=num_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liSL27S131ke"
   },
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    data: Data,\n",
    "    text_transform,\n",
    "    label_transform,\n",
    "    lookups,\n",
    ") :\n",
    "\n",
    "    datasets_dict = {}\n",
    "\n",
    "    train_data = data.train\n",
    "    val_data = data.val\n",
    "    test_data = data.test\n",
    "    del data.df\n",
    "\n",
    "    datasets_dict[\"train\"] = HuggingfaceDataset(\n",
    "        train_data,\n",
    "        split_name=\"train\",\n",
    "        text_transform=text_transform,\n",
    "        label_transform=label_transform,\n",
    "        lookups=lookups,\n",
    "        chunk_size = 128,\n",
    "    )\n",
    "    datasets_dict[\"val\"] = HuggingfaceDataset(\n",
    "        val_data,\n",
    "        split_name=\"val\",\n",
    "        text_transform=text_transform,\n",
    "        label_transform=label_transform,\n",
    "        lookups=lookups,\n",
    "        chunk_size = 128,\n",
    "    )\n",
    "    datasets_dict[\"test\"] = HuggingfaceDataset(\n",
    "        test_data,\n",
    "        split_name=\"test\",\n",
    "        text_transform=text_transform,\n",
    "        label_transform=label_transform,\n",
    "        lookups=lookups,\n",
    "        chunk_size = 128,\n",
    "    )\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0-D_JzW31ke"
   },
   "source": [
    "## Get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "4710c3c6ee9d4c6ebed7cf1d0d534071",
      "e9aecd38c78942b3a966b4fd85ce3c59",
      "4e6a68b8e83e4e55bb5643bda1a5de18",
      "046a2dcef41c458cb0fa3b34904202c3",
      "51849330accc496b93709982b26b9b34",
      "cea73a2900424d438389af3221bcbd76"
     ]
    },
    "id": "8KY-LCfA31kf",
    "outputId": "de884e0f-5374-49fc-c7ac-8afde8ac9d99"
   },
   "outputs": [],
   "source": [
    "datasets = get_datasets(\n",
    "        data=data,\n",
    "        text_transform=text_transform,\n",
    "        label_transform=label_transform,\n",
    "        lookups=lookups,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpyeqUBQ31kf"
   },
   "source": [
    "## Dataloader utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCtANrji31kf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "class BySequenceLengthSampler(Sampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        bucket_boundaries: list[int],\n",
    "        batch_size: int = 64,\n",
    "        drop_last: bool = True,\n",
    "    ):\n",
    "        data = dataset.data\n",
    "        self.data_len = len(dataset)\n",
    "        ind_n_len = []\n",
    "        for example_index, (token_ids, _, _, _, _) in enumerate(data):\n",
    "            num_tokens = token_ids.size(0)\n",
    "            ind_n_len.append(\n",
    "                (example_index, num_tokens)\n",
    "            )  # 3rd index of the tuple is the length of the sequence\n",
    "\n",
    "        self.ind_n_len = ind_n_len\n",
    "        self.bucket_boundaries = bucket_boundaries\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if self.drop_last:\n",
    "            print(\n",
    "                \"WARNING: drop_last=True, dropping last non batch-size batch in every bucket ... \"\n",
    "            )\n",
    "\n",
    "        self.boundaries = list(self.bucket_boundaries)\n",
    "        self.buckets_min = torch.tensor([np.iinfo(np.int32).min] + self.boundaries)\n",
    "        self.buckets_max = torch.tensor(self.boundaries + [np.iinfo(np.int32).max])\n",
    "        self.boundaries = torch.tensor(self.boundaries)\n",
    "\n",
    "    def shuffle_tensor(self, t):\n",
    "        return t[torch.randperm(len(t))]\n",
    "\n",
    "    def __iter__(self):\n",
    "        data_buckets = dict()\n",
    "        # where p is the id number and seq_len is the length of this id number.\n",
    "        for p, seq_len in self.ind_n_len:\n",
    "            pid = self.element_to_bucket_id(p, seq_len)\n",
    "            if pid in data_buckets.keys():\n",
    "                data_buckets[pid].append(p)\n",
    "            else:\n",
    "                data_buckets[pid] = [p]\n",
    "\n",
    "        for k in data_buckets.keys():\n",
    "            data_buckets[k] = torch.tensor(data_buckets[k])\n",
    "\n",
    "        iter_list = []\n",
    "        for k in data_buckets.keys():\n",
    "            t = self.shuffle_tensor(data_buckets[k])\n",
    "            batch = torch.split(t, self.batch_size, dim=0)\n",
    "\n",
    "            if self.drop_last and len(batch[-1]) != self.batch_size:\n",
    "                batch = batch[:-1]\n",
    "\n",
    "            iter_list += batch\n",
    "\n",
    "        random.shuffle(\n",
    "            iter_list\n",
    "        )  # shuffle all the batches so they arent ordered by bucket\n",
    "        # size\n",
    "        for batch in iter_list:\n",
    "            yield batch.numpy().tolist()  # as it was stored in an array\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data_len // self.batch_size) + 1 - int(self.drop_last)\n",
    "\n",
    "    def element_to_bucket_id(self, x, seq_length):\n",
    "        valid_buckets = (seq_length >= self.buckets_min) * (\n",
    "            seq_length < self.buckets_max\n",
    "        )\n",
    "        bucket_id = valid_buckets.nonzero()[0].item()\n",
    "\n",
    "        return bucket_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJJWSkPc31kf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_dataloaders(datasets_dict, batch_size = batch_size, max_batch_size = max_batch_size):\n",
    "    dataloaders = {}\n",
    "    train_batch_size = min(batch_size, max_batch_size)\n",
    "    pprint(f\"Train batch size: {train_batch_size}\")\n",
    "\n",
    "    batch_sampler = BySequenceLengthSampler(\n",
    "        dataset=datasets_dict[\"train\"],\n",
    "        batch_size=train_batch_size,\n",
    "        drop_last=True,\n",
    "        bucket_boundaries = [\n",
    "    400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2600, 3000, 4000\n",
    "]\n",
    ",\n",
    "    )\n",
    "    dataloaders[\"train\"] = DataLoader(\n",
    "        datasets_dict[\"train\"],\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=datasets_dict[\"train\"].collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    dataloaders[\"train_val\"] = DataLoader(\n",
    "        datasets_dict[\"train\"],\n",
    "        batch_size=max_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=datasets_dict[\"val\"].collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    dataloaders[\"val\"] = DataLoader(\n",
    "        datasets_dict[\"val\"],\n",
    "        batch_size=max_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=datasets_dict[\"val\"].collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    dataloaders[\"test\"] = DataLoader(\n",
    "        datasets_dict[\"test\"],\n",
    "        batch_size=max_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=datasets_dict[\"test\"].collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvLF-IK431kf"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "W1KlkvAD31kg",
    "outputId": "21efffc4-7dcb-41c4-c1bf-688dfca829e8"
   },
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(datasets_dict=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HktgOk8831kg"
   },
   "source": [
    "## Define Optimizer for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw-sRGFb31kh"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0001, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00neurcX31kh"
   },
   "outputs": [],
   "source": [
    "accumulate_grad_batches = int(max(max_batch_size/batch_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhQdHvRB31kh"
   },
   "source": [
    "## Calculate Number of Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-a9Tlnsi31kh"
   },
   "outputs": [],
   "source": [
    "num_training_steps = (\n",
    "        math.ceil(len(dataloaders[\"train\"]) / accumulate_grad_batches)\n",
    "        * epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPQWFdE231kh"
   },
   "source": [
    "## Define Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX-aWfn031kh"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                                          mode = 'max',\n",
    "                                                          min_lr=0.0001,\n",
    "                                                          factor=0.9,\n",
    "                                                          patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoDIiFi831kh"
   },
   "source": [
    "## Callback Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qsg7bew31ki"
   },
   "outputs": [],
   "source": [
    "class BaseCallback:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on_initialisation_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_val_begin(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_val_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_fit_begin(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def on_fit_end(self, trainer=None):\n",
    "        pass\n",
    "\n",
    "    def log_dict(\n",
    "        self, nested_dict: dict[str, dict[str, torch.Tensor]], epoch: int\n",
    "    ) -> None:\n",
    "        pass\n",
    "\n",
    "    def on_end(self, trainer=None):\n",
    "        pass\n",
    "class SaveBestModelCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prev_best = None\n",
    "        self.split_name = \"val\"\n",
    "        self.target_name = \"all\"\n",
    "        self.metric_name = \"precision@8\"\n",
    "\n",
    "    def on_epoch_end(self, trainer=None):\n",
    "        best_metric = trainer.metric_collections[self.split_name][\n",
    "            self.target_name\n",
    "        ].get_best_metric(self.metric_name)\n",
    "        if self.prev_best is None or best_metric != self.prev_best:\n",
    "            self.prev_best = best_metric\n",
    "            trainer.save_checkpoint(\"best_model.pt\")\n",
    "            print(\"Saved best model\")\n",
    "\n",
    "    def on_fit_end(self, trainer=None):\n",
    "        trainer.load_checkpoint(\"best_model.pt\")\n",
    "        print(\"Loaded best model\")\n",
    "\n",
    "class EarlyStoppingCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.split_name = \"val\"\n",
    "        self.target_name = \"all\"\n",
    "        self.metric_name = \"precision@8\"\n",
    "        self.patience = 10\n",
    "        self.counter = 0\n",
    "        self.prev_best = None\n",
    "\n",
    "    def on_epoch_end(self, trainer=None):\n",
    "        \"\"\"On the end of each epoch, test if the validation metric has improved. If it hasn't improved for self.patience epochs, stop training.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer, optional): Trainer class. Defaults to None.\n",
    "        \"\"\"\n",
    "        best_metric = trainer.metric_collections[self.split_name][\n",
    "            self.target_name\n",
    "        ].get_best_metric(self.metric_name)\n",
    "        if self.prev_best is None or best_metric > self.prev_best:\n",
    "            self.prev_best = best_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        if self.counter >= self.patience:\n",
    "            trainer.stop_training = True\n",
    "            print(\n",
    "                f\"Early stopping: {self.counter} epochs without improvement for {self.metric_name}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a2pLaI231ki"
   },
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    callbacks_list = []\n",
    "    callbacks_list.append(SaveBestModelCallback())\n",
    "    callbacks_list.append(EarlyStoppingCallback())\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDtuRX8V31ki"
   },
   "source": [
    "## Get the Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFX0-9kG31ki"
   },
   "outputs": [],
   "source": [
    "callbacks = get_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPT6M-X131kj"
   },
   "source": [
    "## Trainer Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMwzOPK-31kj"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs,\n",
    "        data,\n",
    "        model,\n",
    "        optimizer,\n",
    "        dataloaders,\n",
    "        metric_collections,\n",
    "        callbacks,\n",
    "        lr_scheduler = None,\n",
    "        lookups = None,\n",
    "        accumulate_grad_batches = 1,\n",
    "    ) :\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.callbacks = callbacks\n",
    "        self.device = \"cpu\"\n",
    "        self.metric_collections = metric_collections\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lookups = lookups\n",
    "        self.accumulate_grad_batches = accumulate_grad_batches\n",
    "        pprint(f\"Accumulating gradients over {self.accumulate_grad_batches} batch(es).\")\n",
    "        self.validate_on_training_data = True\n",
    "        self.print_metrics = True\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "        self.use_amp = True\n",
    "        self.threshold_tuning = False\n",
    "        self.gradient_scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
    "        self.experiment_path = Path(mkdtemp())\n",
    "        self.current_val_results = None\n",
    "        self.stop_training = False\n",
    "        self.best_db = 0.5\n",
    "        self.val_f1_micro_list = []\n",
    "        self.val_precision_micro_list = []\n",
    "        self.val_precision_1_list = []\n",
    "        self.val_precision_5_list = []\n",
    "        self.val_precision_8_list = []\n",
    "        self.val_precision_15_list = []\n",
    "        self.train_f1_micro_list = []\n",
    "        self.train_precision_micro_list = []\n",
    "        self.train_precision_1_list = []\n",
    "        self.train_precision_5_list = []\n",
    "        self.train_precision_8_list = []\n",
    "        self.train_precision_15_list = []\n",
    "        self.on_initialisation_end()\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"Train and validate the model.\"\"\"\n",
    "        try:\n",
    "            self.on_fit_begin()\n",
    "\n",
    "            for _ in range(self.epoch, self.epochs):\n",
    "                if self.stop_training:\n",
    "                    break\n",
    "                self.on_epoch_begin()\n",
    "                self.train_one_epoch(self.epoch)\n",
    "                if self.validate_on_training_data:\n",
    "                    self.train_val(self.epoch, \"train_val\")\n",
    "                self.val(self.epoch, \"val\")\n",
    "                self.on_epoch_end()\n",
    "                self.epoch += 1\n",
    "            self.on_fit_end()\n",
    "            self.val(self.epoch, \"val\", evaluating_best_model=True)\n",
    "            self.val(self.epoch, \"test\", evaluating_best_model=True)\n",
    "            self.save_final_model()\n",
    "        except KeyboardInterrupt:\n",
    "            pprint(\"Training interrupted by user. Stopping training\")\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.on_end()\n",
    "\n",
    "    def train_one_epoch(self, epoch: int) -> None:\n",
    "        \"\"\"Train the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.on_train_begin()\n",
    "        num_batches = len(self.dataloaders[\"train\"])\n",
    "        for batch_idx, batch in enumerate(\n",
    "            track(self.dataloaders[\"train\"], description=f\"Epoch: {epoch} | Training\")\n",
    "        ):\n",
    "            batch = batch.to(self.device)\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=self.use_amp\n",
    "            ):\n",
    "                output = self.model.training_step(batch)\n",
    "                loss = output[\"loss\"] / self.accumulate_grad_batches\n",
    "            self.gradient_scaler.scale(loss).backward()\n",
    "            if ((batch_idx + 1) % self.accumulate_grad_batches == 0) or (\n",
    "                batch_idx + 1 == num_batches\n",
    "            ):\n",
    "                self.gradient_scaler.step(self.optimizer)\n",
    "                self.gradient_scaler.update()\n",
    "                if self.lr_scheduler is not None:\n",
    "                    if not isinstance(\n",
    "                        self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "                    ):\n",
    "                        self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            self.update_metrics(output, \"train\")\n",
    "        self.on_train_end(epoch)\n",
    "\n",
    "    def train_val(self, epoch, split_name: str = \"train_val\") -> None:\n",
    "        \"\"\"Validate on the training data. This is useful for testing for overfitting. Due to memory constraints, we donÃ¸t save the outputs.\n",
    "\n",
    "        Args:\n",
    "            epoch (_type_): _description_\n",
    "            split_name (str, optional): _description_. Defaults to \"train_val\".\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.on_val_begin()\n",
    "        with torch.no_grad():\n",
    "            for batch in track(\n",
    "                self.dataloaders[split_name],\n",
    "                description=f\"Epoch: {epoch} | Validating on training data\",\n",
    "            ):\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=self.use_amp\n",
    "                ):\n",
    "                    output = self.model.validation_step(batch.to(self.device))\n",
    "                self.update_metrics(output, split_name)\n",
    "            self.on_val_end(split_name, epoch)\n",
    "\n",
    "    def val(\n",
    "        self, epoch, split_name: str = \"val\", evaluating_best_model: bool = False\n",
    "    ) -> None:\n",
    "        self.model.eval()\n",
    "        self.on_val_begin()\n",
    "        logits = []\n",
    "        targets = []\n",
    "        logits_cpu = []\n",
    "        targets_cpu = []\n",
    "        ids = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(\n",
    "                track(\n",
    "                    self.dataloaders[split_name],\n",
    "                    description=f\"Epoch: {epoch} | Validating on {split_name}\",\n",
    "                )\n",
    "            ):\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=self.use_amp\n",
    "                ):\n",
    "                    output = self.model.validation_step(batch.to(self.device))\n",
    "                self.update_metrics(output, split_name)\n",
    "                logits.append(output[\"logits\"])\n",
    "                targets.append(output[\"targets\"])\n",
    "                ids.append(batch.ids)\n",
    "                if idx % 1000 == 0:\n",
    "                    # move to cpu to save gpu memory\n",
    "                    logits_cpu.append(torch.cat(logits, dim=0).cpu())\n",
    "                    targets_cpu.append(torch.cat(targets, dim=0).cpu())\n",
    "                    logits = []\n",
    "                    targets = []\n",
    "            logits_cpu.append(torch.cat(logits, dim=0).cpu())\n",
    "            targets_cpu.append(torch.cat(targets, dim=0).cpu())\n",
    "\n",
    "            logits = torch.cat(logits_cpu, dim=0)\n",
    "            targets = torch.cat(targets_cpu, dim=0)\n",
    "            ids = torch.cat(ids, dim=0)\n",
    "        self.on_val_end(split_name, epoch, logits, targets, ids, evaluating_best_model)\n",
    "\n",
    "    def update_metrics(self, outputs: dict[str, torch.Tensor], split_name: str) -> None:\n",
    "        for target_name in self.metric_collections[split_name].keys():\n",
    "            self.metric_collections[split_name][target_name].update(outputs)\n",
    "\n",
    "    def calculate_metrics(\n",
    "        self,\n",
    "        split_name: str,\n",
    "        logits: Optional[torch.Tensor] = None,\n",
    "        targets: Optional[torch.Tensor] = None,\n",
    "        evaluating_best_model: bool = False,\n",
    "    ) -> dict[str, dict[str, torch.Tensor]]:\n",
    "        results_dict = defaultdict(dict)\n",
    "        if split_name == \"val\":\n",
    "            for target_name in self.metric_collections[split_name].keys():\n",
    "                results_dict[split_name][target_name] = self.metric_collections[\n",
    "                    split_name\n",
    "                ][target_name].compute()\n",
    "        else:\n",
    "            for target_name in self.metric_collections[split_name].keys():\n",
    "                results_dict[split_name][target_name] = self.metric_collections[\n",
    "                    split_name\n",
    "                ][target_name].compute(logits, targets)\n",
    "\n",
    "        if self.threshold_tuning and split_name == \"val\":\n",
    "            best_result, best_db = f1_score_db_tuning(logits, targets)\n",
    "            results_dict[split_name][\"all\"] |= {\"f1_micro_tuned\": best_result}\n",
    "            if evaluating_best_model:\n",
    "                pprint(f\"Best threshold: {best_db}\")\n",
    "                pprint(f\"Best result: {best_result}\")\n",
    "                for target_name in self.metric_collections[\"test\"]:\n",
    "                    self.metric_collections[\"test\"][target_name].set_threshold(best_db)\n",
    "            self.best_db = best_db\n",
    "        return results_dict\n",
    "\n",
    "    def reset_metric(self, split_name: str) -> None:\n",
    "        for target_name in self.metric_collections[split_name].keys():\n",
    "            self.metric_collections[split_name][target_name].reset_metrics()\n",
    "\n",
    "    def reset_metrics(self) -> None:\n",
    "        for split_name in self.metric_collections.keys():\n",
    "            for target_name in self.metric_collections[split_name].keys():\n",
    "                self.metric_collections[split_name][target_name].reset_metrics()\n",
    "\n",
    "    def on_initialisation_end(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_initialisation_end(self)\n",
    "\n",
    "    def on_fit_begin(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_fit_begin(self)\n",
    "\n",
    "    def on_fit_end(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_fit_end(self)\n",
    "\n",
    "    def on_train_begin(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin()\n",
    "\n",
    "    def on_train_end(self, epoch: int) -> None:\n",
    "        results_dict = self.calculate_metrics(split_name=\"train\")\n",
    "        self.current_train_results = results_dict\n",
    "        results_dict[\"lr\"] = self.optimizer.param_groups[0][\"lr\"]\n",
    "        self.log_dict(results_dict, epoch)\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end()\n",
    "\n",
    "    def on_val_begin(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_val_begin()\n",
    "\n",
    "    def on_val_end(\n",
    "        self,\n",
    "        split_name: str,\n",
    "        epoch: int,\n",
    "        logits: torch.Tensor = None,\n",
    "        targets: torch.Tensor = None,\n",
    "        ids: torch.Tensor = None,\n",
    "        evaluating_best_model: bool = False,\n",
    "    ) -> None:\n",
    "        results_dict = self.calculate_metrics(\n",
    "            split_name=split_name,\n",
    "            logits=logits,\n",
    "            targets=targets,\n",
    "            evaluating_best_model=evaluating_best_model,\n",
    "        )\n",
    "        self.current_val_results = results_dict\n",
    "        self.log_dict(results_dict, epoch)\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_val_end()\n",
    "\n",
    "        if evaluating_best_model:\n",
    "            self.save_predictions(\n",
    "                split_name=split_name, logits=logits, targets=targets, ids=ids\n",
    "            )\n",
    "\n",
    "    def save_predictions(\n",
    "        self,\n",
    "        split_name: str = \"test\",\n",
    "        logits: torch.Tensor = None,\n",
    "        targets: torch.Tensor = None,\n",
    "        ids: torch.Tensor = None,\n",
    "    ):\n",
    "        from time import time\n",
    "\n",
    "        tic = time()\n",
    "        pprint(\"Saving predictions\")\n",
    "        label_transform = self.dataloaders[split_name].dataset.label_transform\n",
    "        code_names = label_transform.get_classes()\n",
    "        logits = logits.numpy()\n",
    "        pprint(\"Building dataframe\")\n",
    "        df = pd.DataFrame(logits, columns=code_names)\n",
    "        pprint(\"Adding targets\")\n",
    "        df[TARGET_COLUMN] = list(map(label_transform.inverse_transform, targets))\n",
    "        pprint(\"Adding ids\")\n",
    "        df[ID_COLUMN] = ids.numpy()\n",
    "        pprint(\"Saving dataframe\")\n",
    "        df.to_feather(self.experiment_path / f\"predictions_{split_name}.feather\")\n",
    "        pprint(\"Saved predictions in {:.2f} seconds\".format(time() - tic))\n",
    "\n",
    "    def on_epoch_begin(self) -> None:\n",
    "        self.reset_metrics()\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_begin(self)\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        self.val_f1_micro_list.append((self.epoch, self.current_val_results['val']['all']['f1_micro']))\n",
    "        self.val_precision_micro_list.append((self.epoch, self.current_val_results['val']['all']['precision_micro']))\n",
    "        self.val_precision_1_list.append((self.epoch, self.current_val_results['val']['all']['precision@1']))\n",
    "        self.val_precision_5_list.append((self.epoch, self.current_val_results['val']['all']['precision@5']))\n",
    "        self.val_precision_8_list.append((self.epoch, self.current_val_results['val']['all']['precision@8']))\n",
    "        self.val_precision_15_list.append((self.epoch, self.current_val_results['val']['all']['precision@15']))\n",
    "        self.train_f1_micro_list.append((self.epoch, self.current_train_results['train']['all']['f1_micro']))\n",
    "        self.train_precision_micro_list.append((self.epoch, self.current_train_results['train']['all']['precision_micro']))\n",
    "        self.train_precision_1_list.append((self.epoch, self.current_train_results['train']['all']['precision@1']))\n",
    "        self.train_precision_5_list.append((self.epoch, self.current_train_results['train']['all']['precision@5']))\n",
    "        self.train_precision_8_list.append((self.epoch, self.current_train_results['train']['all']['precision@8']))\n",
    "        self.train_precision_15_list.append((self.epoch, self.current_train_results['train']['all']['precision@15']))\n",
    "        if self.lr_scheduler is not None:\n",
    "            if isinstance(\n",
    "                self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "            ):\n",
    "                self.lr_scheduler.step(\n",
    "                    self.current_val_results[\"val\"][\"all\"][\"f1_micro\"]\n",
    "                )\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(self)\n",
    "\n",
    "    def on_batch_begin(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_batch_begin()\n",
    "\n",
    "    def on_batch_end(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_batch_end()\n",
    "\n",
    "    def log_dict(\n",
    "        self, nested_dict: dict[str, dict[str, torch.Tensor]], epoch: int\n",
    "    ) -> None:\n",
    "        if self.print_metrics:\n",
    "            self.print(nested_dict)\n",
    "        for callback in self.callbacks:\n",
    "            callback.log_dict(nested_dict, epoch)\n",
    "\n",
    "    def on_end(self) -> None:\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_end()\n",
    "\n",
    "    def print(self, nested_dict: dict[str, dict[str, torch.Tensor]]) -> None:\n",
    "        for split_name in nested_dict.keys():\n",
    "            pprint(nested_dict[split_name])\n",
    "\n",
    "    def to(self, device: str) -> \"Trainer\":\n",
    "        self.model.to(device)\n",
    "        for split_name in self.metric_collections.keys():\n",
    "            for target_name in self.metric_collections[split_name].keys():\n",
    "                self.metric_collections[split_name][target_name].to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def save_checkpoint(self, file_name: str) -> None:\n",
    "        checkpoint = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"scaler\": self.gradient_scaler.state_dict(),\n",
    "            \"epoch\": self.epoch,\n",
    "            \"db\": self.best_db,\n",
    "        }\n",
    "        torch.save(checkpoint, self.experiment_path / file_name)\n",
    "        pprint(\"Saved checkpoint to {}\".format(self.experiment_path / file_name))\n",
    "\n",
    "    def load_checkpoint(self, file_name: str) -> None:\n",
    "        checkpoint = torch.load(self.experiment_path / file_name)\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.gradient_scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "        self.epoch = checkpoint[\"epoch\"]\n",
    "        self.best_db = checkpoint[\"db\"]\n",
    "        pprint(\"Loaded checkpoint from {}\".format(self.experiment_path / file_name))\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs, val_f1_micro = zip(*self.val_f1_micro_list)\n",
    "        _, val_precision_micro = zip(*self.val_precision_micro_list)\n",
    "        _, val_precision_1 = zip(*self.val_precision_1_list)\n",
    "        _, val_precision_5 = zip(*self.val_precision_5_list)\n",
    "        _, val_precision_8 = zip(*self.val_precision_8_list)\n",
    "        _, val_precision_15 = zip(*self.val_precision_15_list)\n",
    "\n",
    "        _, train_f1_micro = zip(*self.train_f1_micro_list)\n",
    "        _, train_precision_micro = zip(*self.train_precision_micro_list)\n",
    "        _, train_precision_1 = zip(*self.train_precision_1_list)\n",
    "        _, train_precision_5 = zip(*self.train_precision_5_list)\n",
    "        _, train_precision_8 = zip(*self.train_precision_8_list)\n",
    "        _, train_precision_15 = zip(*self.train_precision_15_list)\n",
    "\n",
    "        # Plotting Training Metrics\n",
    "        plt.figure(figsize=(10, 6))  # Set figure size\n",
    "        plt.plot(epochs, train_f1_micro, label='F1 Micro', marker='o', color='royalblue', linestyle='-')\n",
    "        plt.plot(epochs, train_precision_micro, label='Precision Micro', marker='s', color='darkorange', linestyle='--')\n",
    "        plt.plot(epochs, train_precision_1, label='Precision 1', marker='^', color='forestgreen', linestyle='-.')\n",
    "        plt.plot(epochs, train_precision_5, label='Precision 5', marker='D', color='firebrick', linestyle=':')\n",
    "        plt.plot(epochs, train_precision_8, label='Precision 8', marker='P', color='mediumslateblue', linestyle='-')\n",
    "        plt.plot(epochs, train_precision_15, label='Precision 15', marker='X', color='darkgoldenrod', linestyle='--')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Value', fontsize=14)\n",
    "        plt.title('Training Metrics vs. Epoch', fontsize=16)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend(fontsize=12)  # Add legend with increased font size\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plotting Validation Metrics\n",
    "        plt.figure(figsize=(10, 6))  # Set figure size\n",
    "        plt.plot(epochs, val_f1_micro, label='F1 Micro', marker='o', color='royalblue', linestyle='-')\n",
    "        plt.plot(epochs, val_precision_micro, label='Precision Micro', marker='s', color='darkorange', linestyle='--')\n",
    "        plt.plot(epochs, val_precision_1, label='Precision 1', marker='^', color='forestgreen', linestyle='-.')\n",
    "        plt.plot(epochs, val_precision_5, label='Precision 5', marker='D', color='firebrick', linestyle=':')\n",
    "        plt.plot(epochs, val_precision_8, label='Precision 8', marker='P', color='mediumslateblue', linestyle='-')\n",
    "        plt.plot(epochs, val_precision_15, label='Precision 15', marker='X', color='darkgoldenrod', linestyle='--')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Value', fontsize=14)\n",
    "        plt.title('Validation Metrics vs. Epoch', fontsize=16)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend(fontsize=12)  # Add legend with increased font size\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_transforms(self) -> None:\n",
    "        \"\"\"Save text tokenizer and label encoder\"\"\"\n",
    "        self.dataloaders[\"train\"].dataset.text_transform.save(self.experiment_path)\n",
    "        self.dataloaders[\"train\"].dataset.label_transform.save(self.experiment_path)\n",
    "\n",
    "    def save_final_model(self) -> None:\n",
    "        self.save_checkpoint(\"final_model.pt\")\n",
    "        self.save_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec4bmC7i31kj"
   },
   "source": [
    "## Get the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "1XYi050C31kj",
    "outputId": "f4fc4614-cab7-4f1f-dec5-d5c90e2acaa1"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        epochs = epochs,\n",
    "        data=data,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataloaders=dataloaders,\n",
    "        metric_collections=metric_collections,\n",
    "        callbacks=callbacks,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lookups=lookups,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Nd49Vnk31kk"
   },
   "source": [
    "## Set Experiment Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlZe4d8n31kk"
   },
   "outputs": [],
   "source": [
    "trainer.experiment_path = pretrained_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydW2clZR31kk"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "50d621e78cc5401694c2489f992a6a48",
      "20a0aa6ce77445d3a53d7cc43fcc7809"
     ]
    },
    "id": "X4yBxlzZ31kk",
    "outputId": "29e18a49-b41c-41a6-e908-ec06498d4734"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.autocast(\"cuda\"):\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E74T3Jz831kk"
   },
   "source": [
    "## Visualize the Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfuOREqV31kk"
   },
   "outputs": [],
   "source": [
    "val_preds = pd.read_feather(\"/kaggle/working/RoBERTa-base-PM-M3-Voc-hf/predictions_val.feather\")\n",
    "val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hIYlqzn31kk"
   },
   "source": [
    "## Visualize Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRgjtw2i31kk"
   },
   "outputs": [],
   "source": [
    "val_preds = pd.read_feather(\"/kaggle/working/RoBERTa-base-PM-M3-Voc-hf/predictions_test.feather\")\n",
    "val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1-dTn5B31kk"
   },
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046a2dcef41c458cb0fa3b34904202c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20a0aa6ce77445d3a53d7cc43fcc7809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4710c3c6ee9d4c6ebed7cf1d0d534071": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_e9aecd38c78942b3a966b4fd85ce3c59",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating examples <span style=\"color: #729c1f; text-decoration-color: #729c1f\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n",
         "text/plain": "Creating examples \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "tabbable": null,
      "tooltip": null
     }
    },
    "4e6a68b8e83e4e55bb5643bda1a5de18": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_046a2dcef41c458cb0fa3b34904202c3",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating examples <span style=\"color: #729c1f; text-decoration-color: #729c1f\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n",
         "text/plain": "Creating examples \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "tabbable": null,
      "tooltip": null
     }
    },
    "50d621e78cc5401694c2489f992a6a48": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_20a0aa6ce77445d3a53d7cc43fcc7809",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch: 0 | Training <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">  1%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:05:20</span>\n</pre>\n",
         "text/plain": "Epoch: 0 | Training \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:05:20\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "tabbable": null,
      "tooltip": null
     }
    },
    "51849330accc496b93709982b26b9b34": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_cea73a2900424d438389af3221bcbd76",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating examples <span style=\"color: #729c1f; text-decoration-color: #729c1f\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n",
         "text/plain": "Creating examples \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "tabbable": null,
      "tooltip": null
     }
    },
    "77b0fc3528174d0b8f75502530a4f9d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c80a736fbda340c587a54019e5ef61e5": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_77b0fc3528174d0b8f75502530a4f9d3",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Transforming text... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:04</span>\n</pre>\n",
         "text/plain": "Transforming text... \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:04\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "tabbable": null,
      "tooltip": null
     }
    },
    "cea73a2900424d438389af3221bcbd76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9aecd38c78942b3a966b4fd85ce3c59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
